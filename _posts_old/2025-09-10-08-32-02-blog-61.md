---
layout: post 
title: Why Do We Take LLMs Seriously as a Potential Source of Biorisk?
blog_url: https://red.anthropic.com/2025/biorisk/?utm_source=tldrai 
---

## Overview

Anthropic details its rationale for implementing ASL-3 safety protections against biological weapons development, highlighting that Claude now surpasses expert performance in virology evaluations. Controlled trials demonstrated a significant uplift with Claude Opus 4 for acquiring bioweapons compared to using only the open internet, underscoring the high impact, albeit low-probability, risk that necessitates serious evaluation and restriction.

## Key Points

- Anthropic activated ASL-3 safety protections for Claude Opus 4 due to its potential to assist in CBRN weapons development.
- LLMs, particularly Claude, are demonstrating knowledge and performance in biological sciences nearing or exceeding human experts.
- Controlled trials showed Claude Opus 4 significantly improved participants' ability to plan bioweapons acquisition compared to internet-only access.
- The dual-use nature of AI and the severe consequences of biological attacks make biorisk a critical concern.
- Anthropic is investing in further wet lab studies to understand the role of tacit knowledge and AI's uplift potential in real laboratory settings.
- Public-private partnerships and transparency norms are crucial for optimizing biorisk evaluation and mitigation.

## Key Topics Discussed

Anthropic's article, "Why Do We Take LLMs Seriously as a Potential Source of Biorisk?", elaborates on the company's decision to implement AI Safety Level 3 (ASL-3) protections, specifically targeting the potential misuse of its Claude Opus 4 model in developing chemical, biological, radiological, and nuclear (CBRN) weapons. The core argument rests on the observation that frontier large language models (LLMs), despite being generalists, have acquired extensive knowledge in biological sciences, with Claude now exhibiting performance that, in some virology evaluations, surpasses that of human experts.

The company conducted controlled trials to assess AI's capability to assist in bioweapons acquisition planning. These trials revealed that participants with access to Claude Opus 4 developed significantly more comprehensive plans with fewer critical failures compared to a control group using only the internet. This "uplift" demonstrates the plausible risk of AI empowering malicious actors, even if it's a low-probability, high-impact scenario.

Anthropic emphasizes that biorisk is particularly concerning due to the severe, widespread consequences of biological attacks and the decreasing material barriers in biotechnology. AI models further lower informational barriers, making this combination especially dangerous. The article also discusses the ongoing research, including future wet lab studies co-sponsored by the Frontier Model Forum, to better understand the role of tacit knowledge in laboratory settings and AI's uplift potential in real laboratory studies.

Finally, Anthropic stresses the importance of public-private partnerships and transparency norms, such as secure development frameworks and publishing evaluation results for dangerous capabilities, to effectively evaluate and mitigate biorisk. The company's monitoring activities and collaboration with government bodies like the US Center for AI Standards and Innovation (CAISI) and the UK AI Security Institute are highlighted as crucial steps in building resilience against biological threats.

