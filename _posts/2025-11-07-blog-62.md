---
layout: post 
title: "The Company Quietly Funneling Paywalled Articles to AI Developers"
blog_url: https://www.theatlantic.com/technology/2025/11/common-crawl-ai-training-data/684567/?gift=iWa_iB9lkw4UuiWbIbrWGQv84IP0_-K67yuVC013Fx4&amp;utm_source=tldrai 
---



## Key Points

Common Crawl Foundation, a nonprofit, scrapes billions of webpages, including paywalled articles, to build a massive internet archive.
This archive is used by major AI companies like OpenAI, Google, Anthropic, Nvidia, Meta, and Amazon to train large language models (LLMs).
Common Crawl claims it only scrapes "freely available content" and complies with requests to remove content, but investigation shows this is not true.
Publishers, including The New York Times, have requested content removal, but articles remain in Common Crawl's "immutable" archives.
Common Crawl's executive director, Rich Skrenta, argues that AI models should have access to anything on the internet, framing it as "robot rights."
The foundation has become more involved with the AI industry, receiving donations and helping assemble AI-training datasets.
Common Crawl also provides misleading search results on its website, showing "no captures" for domains that are actually present in its archives.
Skrenta has dismissed suggestions for attribution requirements and downplayed the importance of original reporting from publishers.
The article suggests Common Crawl and the AI industry's actions are detrimental to the "open web" by forcing publishers to strengthen paywalls.

## Key Topics Discussed

Alright, podcast listeners, buckle up, because we've got a fascinating and frankly, pretty concerning, piece of investigative journalism to dive into today from The Atlantic. It's all about a lesser-known nonprofit called the Common Crawl Foundation, and how they've been quietly, yet massively, impacting the AI landscape. For over a decade, Common Crawl has been building an enormous archive of the internet, scraping billions of webpages. And here's the kicker: this archive, which they claim is for research and made freely available, is a goldmine for major AI companies like OpenAI, Google, Anthropic, and Meta. They're all using it to train their large language models. But the investigation reveals a major problem: Common Crawl has been routinely collecting paywalled articles from big news organizations, even though they publicly state they only gather "freely available content" and comply with requests to remove copyrighted material. The article exposes that this simply isn't true. Publishers, including The New York Times and a Danish rights organization, have asked for their content to be taken down, but much of it is still sitting in Common Crawl's archives. Their executive director, Rich Skrenta, even admitted that their file format is basically "immutable," making deletion incredibly difficult. Talk about a contradiction! Skrenta takes a pretty controversial stance, arguing that AI models should have unrestricted access to everything online, even framing it as a matter of "robot rights." He's also apparently dismissed the value of original reporting from publishers. The article also points out that Common Crawl's own website search function is misleading, showing no results for domains that are, in fact, present in their archives. This has led some publishers to falsely believe their content was removed. Plus, Common Crawl has been getting cozy with the AI industry, receiving donations and actively helping to curate AI training datasets. The author suggests that these actions by Common Crawl, and the broader AI industry's argument of "fair use" for copyrighted material, are actually hurting the "open web." Instead of promoting openness, it's forcing publishers to put up stronger paywalls to protect their work and their business models from these exploitative scraping practices. It really makes you think about who truly benefits when information is declared to be 'free' on the internet.

