---
layout: post 
title: "How we built a custom vision LLM to improve document processing at Grab"
blog_url: https://engineering.grab.com/custom-vision-llm-at-grab?utm_source=tldrai 
---



## Key Points

Grab developed a custom Vision LLM to enhance document processing, particularly for identification documents in Southeast Asia.
Traditional OCR and proprietary LLMs faced challenges with diverse languages and document formats in SEA, leading Grab to develop a specialized solution.
They selected Qwen2-VL 2B as their base model due to its efficiency, SEA language support, and dynamic image resolution capabilities.
Initial fine-tuning with LoRA was limited, prompting a switch to a two-stage, full-parameter fine-tuning approach, which significantly improved accuracy for non-Latin scripts like Thai and Vietnamese.
Grab then built a lightweight ~1B parameter Vision LLM from scratch, combining components from larger models, achieving comparable accuracy to the 2B model but with much lower latency.
Key insights include the superiority of full fine-tuning for specialized domains, the effectiveness of lightweight custom models, the importance of base models with native language support, and the critical role of high-quality data.
The custom Vision LLM can replace traditional OCR pipelines with a single, unified, highly accurate model, promising scalable document processing.

## Key Topics Discussed

Alright folks, let's dive into some cutting-edge tech from Grab! They've been hard at work developing a custom Vision Large Language Model, or LLM, to really supercharge their document processing, especially when it comes to things like ID cards across Southeast Asia. Now, as you can imagine, this region is a melting pot of languages and document formats, which presented a huge hurdle for old-school Optical Character Recognition, or OCR, systems and even some of the big-name proprietary LLMs out there. They just weren't cutting it in terms of accuracy, speed, or understanding those unique SEA languages. So, Grab decided to roll up their sleeves and build something tailored.

They started by picking Qwen2-VL 2B as their foundational multimodal LLM. Why this one? Well, it's efficient, supports languages like Thai and Vietnamese, and can handle images in their original resolution, which is a massive plus for keeping text clear in OCR tasks. But, just like any good engineering journey, there were bumps. Initial tests showed the accuracy wasn't quite there, especially for those non-Latin scripts.

Their first attempt at fine-tuning involved something called Low-Rank Adaptation, or LoRA, and while it showed some promise for documents using Latin script, it struggled with the more complex, non-Latin layouts. This led them to a crucial pivot: full-parameter fine-tuning. Drawing inspiration from the LLAVA methodology, they implemented a two-stage training process. First, they pre-trained the vision components using synthetic OCR datasets specifically for Bahasa Indonesia, Thai, Vietnamese, and English. Then, they fine-tuned the entire model with their real-world, task-specific document data. This comprehensive approach delivered a massive boost in accuracy for those tricky Thai and Vietnamese documents!

But Grab didn't stop there. To optimize resources and get an even more perfectly tailored model, they went a step further and built a lightweight Vision LLM from the ground up, clocking in at around 1 billion parameters. They cleverly combined the powerful vision encoder from the larger Qwen2-VL 2B with a more compact language decoder from Qwen2.5 0.5B, making sure they communicated seamlessly. This custom model went through a rigorous four-stage training process. The outcome? This lighter model achieved accuracy comparable to its larger 2B counterpart, but with significantly reduced latency â€“ a huge win for large-scale deployments, especially when you compare it to the inconsistent speeds of external APIs like ChatGPT or Gemini.

Grab's journey offers some fantastic takeaways: for specialized domains with non-Latin scripts, full-parameter fine-tuning is a game-changer. Lightweight, custom-built models can achieve near state-of-the-art results. The base model's native language support is absolutely crucial, and, as always, high-quality, meticulously preprocessed data is king. This whole endeavor really shows that a specialized Vision LLM can consolidate and even outperform traditional OCR pipelines, paving the way for incredibly accurate and efficient document processing at scale. And what's next for Grab? They're looking into smarter, more adaptable Chain of Thought-based OCR models and expanding this awesome tech across all their markets in Southeast Asia. Pretty cool, right?

