---
layout: post 
title: "Two different tricks for fast LLM inference"
blog_url: https://www.seangoedecke.com/fast-llm-inference/?utm_source=tldrai 
---



## Key Points

Anthropic and OpenAI both offer 'fast mode' for their LLMs, but with very different implementations and performance characteristics
Anthropic's fast mode offers ~2.5x speed (170 tokens/sec vs 65 tokens/sec for Opus 4.6) and costs 6x more, likely using low-batch-size inference
OpenAI's fast mode offers >1000 tokens/sec (15x faster than GPT-5.3-Codex's 65 tokens/sec) using GPT-5.3-Codex-Spark model on Cerebras chips
Anthropic's fast mode serves their actual Opus 4.6 model, while OpenAI's serves a less capable GPT-5.3-Codex-Spark distil model
Anthropic's approach is similar to 'guaranteeing a bus leaves immediately' - paying for lower batch size to reduce wait times
OpenAI's approach uses Cerebras chips with 70 square inches (vs H100's ~1 square inch) and 44GB of SRAM to keep model weights in fast memory
The author speculates Anthropic rushed their fast mode to compete with OpenAI's upcoming Cerebras-based announcement
The author finds 'fast, less-capable inference' not very useful, as AI agent usefulness depends more on accuracy than speed
There's discussion about whether fast inference will become a major focus for AI labs or just experimental features
The article explores technical concepts like continuous batching, speculative decoding, and chip-to-chip communication bottlenecks
