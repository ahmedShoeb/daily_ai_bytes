---
layout: post 
title: "The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix"
blog_url: https://huggingface.co/blog/codelion/optimal-dataset-mixing?utm_source=tldrai 
---



## Key Points

Achieving 90%+ GPT-2 performance with 1/10th the training data by optimizing pre-training datasets.
A static mixture of 50% finePDFs, 30% DCLM-baseline, and 20% FineWeb-Edu consistently outperforms complex curriculum strategies.
This optimal mix balances high-quality, structured language from finePDFs with the diversity of web content from DCLM-baseline and educational web resources from FineWeb-Edu.
Curriculum learning with hard transitions between data distributions can lead to catastrophic forgetting or overfitting.
The resulting `codelion/gpt-2-70m` model, trained on 1 billion tokens, achieved 38.15% average score on the HuggingFace Open LLM Leaderboard, only 0.98 percentage points behind the original GPT-2 (trained on 10x more data).
The study highlights that dataset composition is more crucial than total data volume for efficient language model training.

## Key Topics Discussed

Alright folks, get ready to have your minds blown about how we can train smarter, not harder, in the world of large language models! This article from Hugging Face dives deep into Codelion's research on optimizing pre-training datasets to get top-notch performance with significantly less data. Imagine getting over 90% of GPT-2's performance using just a tenth of the training data! That's what they set out to do with their 1 billion token challenge. 

They ran over 50 systematic experiments, playing around with different mixes of three data sources: finePDFs, which are super high-quality textbook-style documents; DCLM-baseline, which brings in diverse web content; and FineWeb-Edu, offering curated educational web resources. And guess what? They found the 'goldilocks zone' – a static mixture of 50% finePDFs, 30% DCLM-baseline, and 20% FineWeb-Edu. This wasn't just good; it consistently beat out more complex curriculum learning strategies, giving them excellent in-domain performance and fantastic generalization.

One of the most eye-opening discoveries was the trade-off between how well a model fits its training data and how well it generalizes to new, unseen data. They learned that pure synthetic data might give you amazing in-domain scores, but it crashes and burns when trying to generalize. On the flip side, too much diverse web data made it harder for the model to learn clear patterns. Their 50-30-20 mix found that perfect sweet spot.

And here's the kicker: they initially thought curriculum learning—gradually changing the data distribution during training, kind of like how humans learn—would be the winner. But nope! Static mixing, where the model sees a consistent data distribution from start to finish, actually performed better. Hard transitions between different data types in curriculum learning led to some pretty serious issues, like 'catastrophic forgetting' or 'overfitting'!

Armed with their optimal 50-30-20 recipe, they trained their `codelion/gpt-2-70m` model on 1 billion tokens. The results are seriously impressive! This model achieved an average score of 38.15% on the HuggingFace Open LLM Leaderboard. That's only a tiny 0.98 percentage points behind the original GPT-2, which was trained on ten times more data and had more parameters! Not only that, but their model actually *won* on the TruthfulQA benchmark. 

So, the big takeaway here is that dataset composition is absolutely crucial. It's not just about how much data you throw at a model, but *what kind* of data and *how* it's mixed. With smart curation, you can achieve over 90% of a model's performance with just 10% of the training data. And the best part? They're sharing both their dataset collection and the trained `codelion/gpt-2-70m` model with the community, so you can try it out yourself!

