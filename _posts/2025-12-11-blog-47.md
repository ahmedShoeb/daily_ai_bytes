---
layout: post 
title: "LLMs Make Legal Advice Lossy"
blog_url: https://writing.kemitchell.com/2025/12/07/LLMs-Make-Legal-Advice-Lossy?utm_source=tldrai 
---



## Key Points

- Clients are using LLMs to summarize legal advice, which the author describes as a 
lossy compression
 process that strips away crucial detail and nuance.
- Lawyers carefully select the level of abstraction for their advice; LLM summaries bypass this, often leading to oversimplified or misleading information.
- Examples like copyright ownership illustrate how fundamental legal rules have critical exceptions and contexts that are lost in automated summaries.
- LLM summarization can also lead to problematic rewording, where important legal terminology and jargon (essential for client understanding and future communication) are altered or removed.
- The author emphasizes the need for clients to inform their lawyers when using LLMs for summarization, allowing lawyers to adapt communication and ensure clients receive full, accurate advice.

## Key Topics Discussed

Alright everyone, let's talk about something that's really shaking up the legal world: the use of Large Language Models, or LLMs, by clients to summarize legal advice. Our esteemed author, K. Mitchell, draws a fantastic analogy here, comparing it to the 'lossy compression' of old digital images. Remember those blurry, pixelated photos we used to see? Well, he argues that LLM summaries are doing something similar to legal advice—stripping away vital detail and nuance for the sake of brevity. It’s like getting a quick sketch when you really need a detailed blueprint! Mitchell, as a lawyer, highlights that a crucial part of his job is carefully crafting advice at the right level of abstraction, making complex legal rules digestible without losing their essence. But when clients then feed this into a chatbot, they're essentially overriding that careful calibration. This can lead to summaries that are too terse, potentially misleading, and even fraught with legal risks. He gives a great example with copyright ownership: while it seems straightforward that 'authors own the rights in their works,' in a business context, that's a dangerous oversimplification. There are tons of exceptions, like 'work made for hire' or the right to terminate old deals, that simply vanish in a high-level, AI-generated summary. Beyond just losing information, there's another fascinating point about rewording. When an LLM summarizes a summary, it often rephrases things. And in law, specific word choices and the introduction of precise jargon are super important—not just for current understanding, but for how clients communicate externally and navigate future legal situations. When an LLM takes over the language, the lawyer loses control over this critical aspect of client education. Now, Mitchell isn't saying it's all bad. He recognizes the pressure to write in shorter, more organized ways, especially with today's fast-paced communication styles. But his main takeaway is this: transparency is key! Clients absolutely need to tell their lawyers if they're using chatbots to summarize advice. This allows the lawyer to adjust, ensure the client fully grasps the information, and prevent any costly misunderstandings that could arise from these 'lossy' AI summaries. Ultimately, it’s about both lawyers adapting to clearer writing and clients being open about their tech usage to maintain the integrity of legal counsel.

