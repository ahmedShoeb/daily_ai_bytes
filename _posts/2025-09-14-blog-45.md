---
layout: post 
title: Writing Effective Tools for Agents — With Agents
blog_url: https://www.anthropic.com/engineering/writing-tools-for-agents?utm_source=tldrai 
---

## Overview

Anthropic engineers found that AI agents perform better with fewer, more thoughtful tools. They also discovered that Claude-optimized tools significantly outperformed human-written versions, as agents could improve their own toolsets through evaluation loops.

## Key Points

- Agents perform better with fewer, thoughtful tools.
- Claude-optimized tools outperformed human-written tools.
- Agents can improve their own toolsets through evaluation loops.
- The article provides techniques for improving agent performance with tools.

## Key Topics Discussed

This article, 'Writing Effective Tools for Agents — With Agents,' by Anthropic engineers, explores methods for optimizing tool utilization by AI agents. A key finding is that agents achieve superior performance when equipped with a smaller number of carefully designed tools, as opposed to a broad array that merely wraps every API endpoint. The most compelling insight is the demonstration that tools optimized by Claude itself consistently surpassed human-written versions in internal tests. This was achieved through iterative evaluation loops where agents could autonomously refine and enhance their own toolsets. The article outlines a practical framework for building and testing tool prototypes, conducting comprehensive evaluations, and leveraging agents like Claude Code for collaborative improvement. It also distills key principles for effective tool design, such as selecting high-impact workflows, judiciously naming tools to delineate functionality, returning meaningful and token-efficient context, and prompt-engineering tool descriptions for clarity. The overarching message is a call to fundamentally rethink software development practices for non-deterministic AI agents, focusing on intentional design and continuous, agent-driven refinement.

