---
layout: post 
title: "Anatomy of a Finetuning API"
blog_url: https://benanderson.work/blog/anatomy-of-finetuning-api/?utm_source=tldrai 
---



## Key Points

- Thinking Machines released Tinker, a language model fine-tuning API.
- Tinker offers low-level functions like `sample`, `forward_backward`, and `optim_step` for fine-tuning and online reinforcement learning.
- Unlike traditional methods, Tinker's design seemingly introduces network latency per training step.
- The author theorizes this approach enables efficient multi-tenancy by using LoRA adapters with a shared pool of open-source base models.
- LoRA adapters allow for rapid switching between different user models, sharing compute resources effectively.
- This architecture could provide insights into how large AI labs manage numerous custom models with low latency.
- Tinker aims to provide stable infrastructure for RL and fine-tuning experiments, making AI research more accessible.

## Key Topics Discussed

The article delves into Thinking Machines' newly launched language model fine-tuning API, Tinker, contrasting it with conventional fine-tuning methodologies. The author, drawing on experience building a prior fine-tuning service, explains that traditional approaches primarily relied on supervised fine-tuning. These methods trained models to mimic predefined responses from datasets, often incorporating techniques like LoRA or QLoRA to optimize memory and performance on GPUs. A key focus in these older systems was minimizing computational waste and maximizing GPU utilization through meticulous data preparation and hardware management.

Tinker, however, takes a distinct path by exposing low-level functions such as `sample`, `forward_backward`, and `optim_step`. This design supports both supervised fine-tuning and, significantly, online reinforcement learning, which is particularly advantageous when evaluating AI-generated responses is more practical than crafting them beforehand. A seemingly counterintuitive aspect of Tinker is the network latency introduced with each training step, a practice typically avoided in deep learning to maintain high GPU throughput.

The author proposes that this unconventional design is, in fact, a clever strategy for scalability. The theory suggests that Tinker is engineered not for single-user, single-model training, but rather for a multitude of users training numerous models concurrently. By leveraging LoRA adapters, which are compact and can be swapped with sub-second latency, Thinking Machines can achieve multi-tenancy. This allows different users' requests to share a warm pool of open-source base models, effectively utilizing GPU resources despite network latency by dynamically switching between LoRA adapters for various users. This architectural insight, the author suggests, might mirror how major AI labs like OpenAI handle serving hundreds of thousands of custom models with minimal latency, by enabling shared inference and potentially training compute.

In conclusion, the article expresses optimism regarding Tinker's potential to democratize AI research by offering a stable and mature infrastructure for fine-tuning and reinforcement learning experiments, thereby making advanced AI tools more accessible to a broader audience.

