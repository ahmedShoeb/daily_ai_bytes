---
layout: post 
title: "ChatGPT 5.1 Codex-Max Review"
blog_url: https://thezvi.wordpress.com/2025/11/25/chatgpt-5-1-codex-max/?utm_source=tldrai 
---



## Key Points

- OpenAI has released GPT-5.1-Codex-Max, an advanced coding model with improved speed, capability, token efficiency, and persistence for long tasks.
- The model achieved significant gains on coding benchmarks: 77.9% on SWE-bench-verified, 79.9% on SWE-Lancer-IC SWE, and 58.1% on Terminal-Bench 2.0.
- GPT-5.1-Codex-Max is designed to be an automated software engineer and is the first model natively trained to operate across multiple context windows using a process called compaction, handling millions of tokens in a single task.
- OpenAI is actively preparing for potential high-level cybersecurity threats posed by the model's capabilities, although external evaluations showed similar or slightly reduced cyberoffensive capabilities compared to GPT-5.
- The model's system card details various features including improved handling of disallowed content, sandboxing, and mitigations for harmful tasks and prompt injections, and it can now operate on Windows.
- While showing progress in AI self-improvement tasks (e.g., SWE-Lancer Diamond from 67% to 80%, OpenAI Proof Q&A from 2% to 8%), the article suggests it's not yet close to fully autonomous AI self-improvement.
- METR evaluations indicate that a significant trend break would be required for rogue replication or AI R&D automation within six months.
- Despite internal improvements, external community reactions to Codex-Max have been minimal, with a general consensus that it's a modest upgrade.
- The author notes early signs that Claude's Opus 4.5 might offer a more substantial upgrade.
- Network access is disabled by default in the sandbox, but users can enable it with associated risks like prompt injection.

## Key Topics Discussed

Alright podcast listeners, let's talk about OpenAI's latest offering, GPT-5.1-Codex-Max! This is their new coding model, and according to the article, it's faster, more capable, and more token-efficient than its predecessor, GPT-5.1-Codex. They've also given it better persistence for those long, complex tasks. We're seeing some pretty impressive benchmark scores here, with substantial gains across the board. For example, it hit 77.9% on SWE-bench-verified and nearly 80% on SWE-Lancer-IC SWE. That's a big jump! 

The big goal for Codex-Max is to be an automated software engineer. And get this: it's their first model natively trained to work across multiple context windows using something called 'compaction.' This means it can coherently handle millions of tokens in a single task, which is a huge deal for complex coding projects. Plus, it's been trained on real-world software engineering tasks like PR creation and code review.

Now, with great power comes great responsibility, right? OpenAI is already getting ready for high-level cybersecurity threats because of this model's capabilities. Interestingly, while their internal tests show big improvements in areas like Capture the Flag, external evaluations from 'Irregular' actually found similar or even slightly reduced cyberoffensive capabilities compared to GPT-5. A bit of a head-scratcher there!

The system card is quite detailed, covering everything from how it handles disallowed content to its sandboxing features. They've even trained it to use Windows, which is a new capability. Default network access is disabled in the sandbox for security, but users can grant full access, though that comes with risks like prompt injection. 

When it comes to AI self-improvement, Codex-Max is making strides. We're seeing jumps in tasks like SWE-Lancer Diamond and OpenAI Proof Q&A. However, the author notes we're not quite at the point of fully autonomous AI self-improvement yet. METR's evaluations suggest that for rogue replication or AI R&D automation within six months, we'd need a significant trend break.

Here's where it gets interesting: despite all these internal advancements, the overall 'organic' reaction from the community to Codex-Max has been pretty minimal. It seems many are seeing it as a modest upgrade. In fact, early whispers suggest that Claude's Opus 4.5 might actually be the bigger upgrade this time around. So, while Codex-Max is a good model and certainly an improvement, it sounds like the AI world is still waiting for that next truly mind-blowing leap. The frog, it is boiling, folks â€“ incremental improvements are happening, but the big splash might be just around the corner!

