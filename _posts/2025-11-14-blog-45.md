---
layout: post 
title: "How to Train an LLM: Part 1"
blog_url: https://omkaark.com/posts/llm-1b-1.html?utm_source=tldrai 
---



## Key Points

The article details the initial phase of training a 1.2 billion parameter Llama 3-style LLM on 8xH100 GPUs, focusing on infrastructure and optimization.
Memory usage for weights, gradients, activations, and optimizer state is analyzed, highlighting activations as the largest consumer.
Debugging revealed that CUDA SDPA with FP32 inputs on H100s led to increased attention activation memory due to fallback to naive attention.
Key optimizations implemented include `torch.compile`, mixed precision with BF16, Fused AdamW, and Gradient Checkpointing to reduce memory and improve throughput.
The author describes a multi-stage training rollout (Mac, 1xH100, 2xH100, 8xH100) to identify and fix bugs efficiently, minimizing costs.
Initial 8xH100 training encountered a stagnant loss after 15,000 steps, suggesting issues with the learning rate schedule, which could not be revived.
Current training efficiency shows an MFU of 18% and throughput of 400k tokens/second, indicating substantial room for improvement in future iterations.
The author also experimented with AdamW8bit, observing a 10GB memory drop and stable convergence, making it a candidate for future runs.

## Key Topics Discussed

Alright podcast listeners, we're diving deep into the world of Large Language Model training today with an article titled 'How to Train an LLM: Part 1.' This piece chronicles the fascinating journey of building a domain-specific LLM, kicking off with a 1.2 billion parameter Llama 3-style model on a powerful 8xH100 GPU cluster. The author walks us through the nitty-gritty of setting up the pre-training infrastructure and tackling some pretty significant challenges in memory management and computational efficiency.

A huge chunk of the discussion revolves around optimizing memory. The author meticulously breaks down how much memory weights, gradients, activations, and optimizer states consume, singling out activations as the biggest memory hog. They even discovered a critical bug on H100s where CUDA's Scaled Dot-Product Attention, when given FP32 inputs, surprisingly fell back to a less efficient 'naive attention' mechanism, leading to unexpectedly high activation memory usage.

To combat these memory woes, a series of clever optimization techniques were put to the test. They leveraged `torch.compile` to fuse operations and cut down on overhead, which definitely boosted throughput. Mixed precision training with BF16 was a game-changer, halving activation memory and allowing for much larger batch sizes. Fused AdamW helped save on optimizer memory, and the most dramatic improvement came from Gradient Checkpointing. This technique cleverly skips storing certain activations during the forward pass and re-computes them when needed during the backward pass. This resulted in an incredible 44GB reduction in reserved memory, though it did slightly increase compute time.

The training process itself wasn't a 'YOLO run,' as the author puts it. It was a careful, staged rollout, starting on a MacBook and gradually scaling up to 1xH100, then 2xH100, and finally the full 8xH100 cluster. This systematic approach was key to catching and squashing bugs early, especially those tricky ones related to `torch.compile` and gradient accumulation, as well as integrating Flash-Attention-3.

Despite all this hard work, the initial 8xH100 training run hit a snag: the loss plateaued after about 15,000 steps. The author suspects an unoptimized learning rate schedule was to blame, highlighting the crucial role of good logging and hyperparameter tuning. Currently, their training infrastructure boasts an MFU (mean flops utilization) of 18% and a throughput of 400k tokens per second. While these are good starting points, there's clearly room for more optimization, which they plan to explore in future blogs. Oh, and they also played around with AdamW8bit, which managed to drop reserved memory by 10GB while maintaining stable convergence â€“ definitely a promising option for future runs! It's a great look into the real-world challenges and iterative nature of pushing the boundaries in LLM training.

