---
layout: post 
title: "Modular Manifolds"
blog_url: https://thinkingmachines.ai/blog/modular-manifolds/?utm_source=tldrai 
---



## Key Points

Neural network training requires healthy tensors (weights, activations, gradients) to avoid numerical issues and improve algorithm design.
Normalization is a key technique for maintaining tensor health, commonly applied to activations and gradients but less so to weight matrices.
The article proposes constraining weight matrices to submanifolds at each layer, enabling co-design of optimization algorithms with these constraints.
It introduces "manifold Muon," an optimizer where weights are restricted to the Stiefel manifold (matrices with unit condition number), derived by choosing a spectral norm as the distance function.
A first-order manifold optimizer involves finding a tangent vector, applying a scaled update, and retracting weights to the manifold.
The concept of "modular manifolds" is presented as an abstraction to budget learning rates across layers by tracking Lipschitz sensitivity and composing modules with specific rules for forward functions, manifold constraints, and norms.
The theory of modular manifolds aims to make scaling and training large networks easier and more principled.
The post concludes by suggesting various directions for future research, including modularity, numerics, convergence analysis, regularization, and non-Riemannian geometry.

## Key Topics Discussed

The article "Modular Manifolds" explores advanced techniques for training large neural networks, specifically focusing on the critical need to maintain the "health" of tensors, including weights, activations, and gradients. Unhealthy tensors, being either too large or too small, can lead to numerical underflow/overflow and complicate the design of training algorithms. While normalization is a standard practice for activations and gradients, its application to weight matrices is less common despite its potential benefits, such as simplifying optimization updates, preventing weight norm explosions, and facilitating Lipschitz guarantees for robustness.

The authors propose an appealing approach: constraining the weight matrices of a neural network to submanifolds at each layer. This strategy allows for the co-design of optimization algorithms with these manifold constraints. As an example, the article introduces "manifold Muon," a novel optimizer designed for weights constrained to the Stiefel manifoldâ€”the manifold of matrices with a unit condition number. The derivation of this optimizer involves understanding the shape of a manifold optimizer, which typically includes finding an optimal tangent vector, applying a learning rate scaled update, and retracting the updated weights back to the manifold. The choice of distance measure in the tangent space, such as the spectral norm for the Stiefel manifold, is crucial in this design.

Furthermore, the post introduces the theory of "modular manifolds," an abstraction aimed at simplifying the scaling and training of large networks by enabling the budgeting of learning rates across layers. This theory is built on the observation that budgeting learning rates is intrinsically linked to the Lipschitz sensitivity of the network's output with respect to its weights. A neural network module is defined by its forward function, a submanifold of its weight space, and a norm. The article details how these modules can be composed following specific rules for their forward functions, manifold constraints (Cartesian product), and norm functions (max of weighted norms) to automatically compile Lipschitz statements and derive separate optimizers for each layer with adjusted learning rates. The piece concludes by highlighting several promising directions for future research in this exciting area, including investigations into modularity, numerics, convex optimization, convergence analysis, regularization, architecture-optimizer co-design, and the exploration of non-Riemannian geometry in machine learning.

