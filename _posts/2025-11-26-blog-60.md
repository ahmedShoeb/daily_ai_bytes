---
layout: post 
title: "How LLM Inference Works"
blog_url: https://arpitbhayani.me/blogs/how-llm-inference-works?utm_source=tldrai 
---



## Key Points

LLMs are neural networks built on the transformer architecture, efficiently processing sequences in parallel.
Transformers use self-attention to evaluate how each word relates to the entire sequence.
Model size refers to the number of parameters that store learned knowledge.
Decoder-only transformers generate one token at a time, autoregressively.
Tokenization converts text into numerical tokens, commonly using Byte Pair Encoding (BPE).
Token embeddings transform token IDs into high-dimensional vectors, with positional encodings added for order.
LLM inference involves two phases: Prefill (parallel processing of prompt) and Decode (sequential token generation).
The KV Cache optimizes inference by storing Key and Value matrices of previous tokens to avoid recomputation.
KV cache significantly improves speed but increases memory usage, especially for long contexts.
Matrix multiplication is a core computational operation, accelerated by GPUs using tiling and Tensor Cores.
Reduced precision formats (FP16, INT8, INT4) and quantization are used to decrease memory and increase inference speed.
Specialized frameworks like vLLM, TensorRT-LLM, and TGI optimize production LLM inference.
Key performance metrics include Time to First Token (TTFT), Inter-Token Latency (ITL), and Throughput.

## Key Topics Discussed

Alright podcast listeners, today we're diving deep into the fascinating world of how Large Language Models, or LLMs, actually perform inference—that's how they generate responses to your prompts! It's more intricate than just magic. 

First off, LLMs are essentially neural networks built on the groundbreaking transformer architecture. Unlike older models that chugged along word by word, transformers can process entire sequences in parallel, making them super efficient. At their heart, these models use something called 'self-attention' to understand how each word relates to every other word in a sentence, not just its immediate neighbors. This powerful mechanism, along with feed-forward networks, makes up the core of each transformer layer. The 'size' of an LLM, like a 7-billion parameter model, refers to the sheer number of learned values it holds from its training.

Most of the LLMs we interact with daily, think GPT-4 or Llama, are 'decoder-only' transformers. This means they are autoregressive, generating one word or 'token' at a time, building on everything they've said before, which is perfect for text generation.

Before any heavy lifting happens, your text prompt needs to be converted into numbers—a process called tokenization. Often using Byte Pair Encoding, text is broken into smaller units called tokens. This is crucial because the number of tokens directly impacts computation cost and speed. These tokens then get transformed into 'embeddings'—high-dimensional vectors that capture semantic meaning. To ensure the model understands the order of your words, positional encodings are also added.

Now, for the actual inference, there are two main acts: the Prefill phase and the Decode phase. When you first hit 'enter' on your prompt, the Prefill phase kicks in. The model processes all your input tokens simultaneously, which is very compute-intensive. This phase is all about crunching numbers fast to give you that crucial 'Time to First Token'—how long you wait before seeing any output. Once that first token is out, we move to the Decode phase. Here, tokens are generated one by one, autoregressively. This part is more memory-intensive, as the GPU is constantly loading data from memory rather than just performing raw calculations.

A huge hero in optimizing this process is the 'KV Cache'. Imagine generating 100 tokens; without this cache, the model would have to re-read and re-process all the previous tokens 100 times! The KV Cache smartly stores the 'Key' and 'Value' matrices for previously generated tokens, so the model only needs to compute for the new token, saving massive amounts of computation. However, this cache can get pretty big, especially with long conversations, putting pressure on memory.

Underneath it all, matrix multiplication is the computational engine. GPUs are brilliant at this, using clever techniques like 'tiling' and specialized 'Tensor Cores' to speed things up. And to further boost performance and fit these massive models onto various hardware, inference often runs at 'reduced precision' (like FP16, INT8, or even INT4) through a process called quantization. This can drastically cut memory usage with minimal impact on output quality.

The entire journey, from your prompt being tokenized, flowing through transformer layers, utilizing the KV cache, and finally being detokenized back into readable text, is a marvel of engineering. And for deploying these LLMs in the real world, specialized frameworks like vLLM, TensorRT-LLM, and Hugging Face's TGI are used to manage everything from batching to memory. Keeping an eye on metrics like Time to First Token, Inter-Token Latency, and Throughput is key to ensuring these models perform smoothly and efficiently. It's a complex dance of computation and memory, but incredibly effective!

