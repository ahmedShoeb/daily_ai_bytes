---
layout: post 
title: "Power Up FSDP2 as a Flexible Training Backend for Miles"
blog_url: https://lmsys.org/blog/2025-12-03-miles-fsdp/?utm_source=tldrai 
---



## Key Points

- FSDP has been integrated into Miles as a more flexible training framework, aligning with Megatron.
- FSDP2, utilizing DTensor, offers superior sharding capabilities and out-of-the-box support for MixedPrecision Training and LoRA, addressing limitations of FSDP1.
- Miles adopts FSDP for its adaptability to complex VLM architectures, agility in supporting new model designs like Qwen3-Next, lower learning curve as a PyTorch native backend, and seamless compatibility with HuggingFace models.
- The architectural design for FSDP in Miles employs "Interface Standardization + Physical Isolation" with Ray Actors to manage different backends and prevent conflicts.
- Key optimizations include Data Packing for efficient token utilization, strict Training-Inference Consistency for accurate log-probability, and algorithmic mitigations like Truncated Importance Sampling (TIS) for mismatch scenarios.
- Weight update strategies are optimized for both colocated and distributed modes using asynchronous updates, and VRAM is conserved through intelligent offloading of model components to the CPU.
- While FSDP shows comparable Context Parallelism performance to Megatron, current limitations include lack of support for Tensor Parallel, Expert Parallel, and Pipeline Parallel, and certain Megatron-specific features.
- Future plans for FSDP include implementing TP and EP, expanding VLM training capabilities, and supporting hybrid models.
- The article provides a quick start guide for setting up the FSDP backend with models like Qwen3-4B.

## Key Topics Discussed

Hey podcast listeners! Today, we're diving into a fascinating development in the world of AI training. The folks behind Miles, a powerful reinforcement learning framework, have just announced a significant upgrade: the integration of FSDP2, or Fully Sharded Data Parallel, as a more flexible training backend. This is a huge deal, especially as it aligns with Megatron's capabilities and opens up new possibilities for advanced model architectures. FSDP, for those not in the know, is a sophisticated evolution of traditional distributed data parallel methods. It intelligently shards model weights, gradients, and optimizer states across multiple GPUs, making training much more efficient. FSDP2, in particular, leverages something called DTensor, which provides even better sharding and natively supports mixed-precision training and LoRA, overcoming some of the complexities of its predecessor, FSDP1. So, why did Miles, an enterprise-focused RL framework, decide to bring FSDP into the fold? Well, there are several compelling reasons. FSDP offers superior adaptability for the intricate architectures of Visual Language Models, or VLMs, and allows for much greater agility when integrating rapidly evolving new architectures, like Qwen3-Next. Plus, as a PyTorch-native backend, it boasts a lower barrier to entry and simplifies debugging. And here's a big one: it's seamlessly compatible with the HuggingFace Model format, which means no more tedious weight conversions â€“ a real time-saver! The way FSDP is integrated into Miles is pretty clever. They've adopted a 

