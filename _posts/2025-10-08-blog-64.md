---
layout: post 
title: "Which Table Format Do LLMs Understand Best? (Results for 11 Formats)"
blog_url: https://www.improvingagents.com/blog/best-input-data-format-for-llms?utm_source=tldrai 
---



## Key Points

- The article investigates the best format for passing tables of data to LLMs, highlighting its importance for system accuracy and token costs.
- A controlled experiment with GPT-4.1-nano tested 11 different data formats using 1000 synthetic employee records and 1000 queries.
- Markdown-KV, a non-standardized format with “key: value” pairs, achieved the highest accuracy at 60.7%.
- Common formats like CSV and JSONL performed poorly in terms of accuracy.
- There is a trade-off between accuracy and token cost; Markdown-KV used 2.7 times more tokens than the most token-efficient format, CSV.
- Simple data transformations based on format choice can significantly improve LLM system accuracy.

## Key Topics Discussed

This article explores the optimal input data format for Large Language Models (LLMs) when processing tabular information, emphasizing its impact on AI system reliability, accuracy, and token costs in RAG pipelines. A controlled experiment was conducted using GPT-4.1-nano, where 1000 synthetic employee records were presented in 11 different formats, and 1000 randomized queries were posed to the model. The results showed considerable differences in LLM understanding across formats. Markdown-KV, a custom format using "key: value" pairs in markdown, emerged as the most accurate with 60.7%. In contrast, widely adopted formats like CSV and JSONL exhibited lower accuracy. The study also revealed a trade-off between accuracy and token consumption; Markdown-KV, despite its superior accuracy, utilized 2.7 times more tokens than the most token-efficient format, CSV. The authors suggest that for paramount accuracy, Markdown-KV is a good default, while markdown tables offer a balance between readability and cost. They caution against the default use of CSV or JSONL due to their poorer performance. The article concludes that strategic data transformations can effectively enhance the accuracy of LLM-based systems, though it notes limitations such as the single LLM model and data pattern used in the experiment.

