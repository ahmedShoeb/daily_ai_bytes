---
layout: post 
title: "Introducing Spreadsheet Arena: A Platform for Evaluating LLMs on Spreadsheet Generation"
blog_url: https://www.meridian.ai/blog/all/spreadsheet-arena?utm_source=tldrai 
---



## Key Points

- Spreadsheet Arena is an open platform released by Meridian AI in collaboration with Cornell, CMU, and Scale AI for evaluating LLMs on spreadsheet workbook generation
- The platform creates blind pairwise battles between model outputs where users vote on which spreadsheet is better without knowing which model produced it
- Thousands of votes have been collected across models from OpenAI, Anthropic, Google, xAI, Meta, Alibaba, and Moonshot
- The evaluation covers diverse prompts spanning professional finance, corporate FP&A, academic research, operations, creative uses, and small business workflows
- Findings show formatting and structure drive user preference more than formula sophistication, with text density, background fills, and numeric content being stronger predictors of winning than lookup functions or conditionals
- Domain-specific patterns emerged: in academic contexts heavy formatting hurts, while in finance professional color-coding conventions provide significant positive signals
- A blinded expert evaluation with finance professionals revealed they only agreed with crowd preferences about half the time, with the biggest gap being around color coding and formatting conventions
- Top models still don't reliably follow real-world financial modeling conventions
- Failure analysis shows losing spreadsheets typically have multiple issues simultaneously, with presentation deficiency being most common across all models
- Model families show different failure signatures: Claude models lose less on polish but more on integrity/numerical correctness, while weaker models struggle with basic prompt compliance
