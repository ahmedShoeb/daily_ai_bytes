---
layout: post 
title: "InferenceMAX™: Real-time LLM Inference Benchmarking"
blog_url: https://github.com/InferenceMAX/InferenceMAX?utm_source=tldrai 
---



## Key Points

InferenceMAX™ is an open-source, Apache2 licensed automated benchmark that continuously tracks the real-time performance of open-source LLM inference frameworks and models.
It addresses the challenge of rapidly evolving AI software by providing up-to-date performance indicators, accessible via a live dashboard at https://inferencemax.ai/.
The project highlights the importance of both hardware and daily software advancements (e.g., SGLang, vLLM, TensorRT-LLM) in driving LLM inference performance.
InferenceMAX™ is supported by major industry leaders, including AMD and NVIDIA, who provided hardware, and various companies offering compute resources, underscoring its collaborative nature.
Industry leaders from OpenAI, Together AI, and vLLM emphasize the critical need for open, transparent, and reproducible benchmarks like InferenceMAX™ to accelerate progress in the AI ecosystem.
The project is actively looking for an engineer to join its special projects team, focusing on large-scale benchmarking across diverse hardware and building reliable CI/CD pipelines.

## Key Topics Discussed

InferenceMAX™ is an innovative open-source project, licensed under Apache2, designed to provide real-time, continuous benchmarking of the world's leading open-source LLM inference frameworks and models. Recognizing that AI software evolves daily with kernel-level optimizations and new strategies, InferenceMAX™ tackles the problem of rapidly outdated benchmarks by constantly re-benchmarking performance. This ensures that the tracked progress accurately reflects the latest software advancements. A live, publicly accessible dashboard at https://inferencemax.ai/ showcases these performance metrics.

The project emphasizes that LLM inference performance is a product of both hardware innovation and continuous software improvements. It extensively benchmarks frameworks such as SGLang, vLLM, TensorRT-LLM, CUDA, and ROCm. InferenceMAX™ benefits from significant industry support, with hardware contributions from AMD (MI355X, CDNA3 GPUs) and NVIDIA (GB200 NVL72 rack, B200 GPUs). Additionally, it acknowledges the efforts of software maintainers like SGLang, vLLM, and TensorRT-LLM, and expresses gratitude to compute providers such as Crusoe, CoreWeave, Nebius, TensorWave, Oracle, and TogetherAI for enabling its operations.

Testimonials from prominent figures like Peter Hoeschele of OpenAI, Tri Dao of Together AI, and Simon Mo of vLLM underscore the project's importance. They highlight that transparent, head-to-head benchmarks are crucial for cutting through noise, reflecting real-world performance, and helping the entire ML community make informed decisions. Such open, reproducible results are seen as vital for accelerating ecosystem progress. InferenceMAX™ is also expanding its team, seeking an engineer passionate about performance engineering and system reliability to work on large-scale, multi-vendor benchmarks and automated CI/CD pipelines, demonstrating its ongoing commitment to advancing AI inference measurement.

