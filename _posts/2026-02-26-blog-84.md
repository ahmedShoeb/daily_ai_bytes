---
layout: post 
title: "Why SWE-bench Verified no longer measures frontier coding capabilities"
blog_url: https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified/?utm_source=tldrai 
---



## Key Points

OpenAI is discontinuing SWE-bench Verified as a benchmark for measuring frontier AI models' coding capabilities
Two major issues were identified: flawed test cases (59.4% of audited problems reject correct solutions) and training data contamination
All frontier models tested (GPT-5.2, Claude Opus 4.5, Gemini 3 Flash) showed evidence of having seen SWE-bench problems during training
Progress on SWE-bench Verified has slowed from 74.9% to 80.9% in the last 6 months
OpenAI recommends using SWE-bench Pro instead, which suffers less from contamination issues
The benchmark's contamination means scores increasingly reflect training exposure rather than genuine coding improvement
OpenAI is building new, uncontaminated evaluations and advocating for privately authored benchmarks like GDPVal
The audit found 35.5% of tasks have overly narrow test cases and 18.8% have overly wide test cases
OpenAI has stopped reporting SWE-bench Verified scores and recommends other developers do the same
