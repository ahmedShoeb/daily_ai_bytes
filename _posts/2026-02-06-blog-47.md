---
layout: post 
title: "The Paradox of Self-Building AI Agents: Teaching AI to Teach Itself"
blog_url: https://foundationcapital.com/the-paradox-of-self-building-agents-teaching-ai-to-teach-itself/ 
---



## Key Points

- AI is transitioning from a simple tool to an autonomous agent that can act, break down goals, and execute tasks independently.
- Foundation Capital predicts that 'agents' will collapse traditional enterprise software stacks, shifting the paradigm from 'software-as-a-service' to 'service-as-software'.
- Yohei Nakajima introduced four levels of AI agent autonomy: Level 0 (fixed tools), Level 1 (request-based self-building), Level 2 (need-based self-building), and Level 3 (anticipatory building).
- Most current AI tools operate at Level 0, relying on fixed capabilities without the ability to create new tools.
- Level 1 agents can generate new tools only when explicitly requested by users, like an intern asking for help.
- Level 2 agents build tools automatically when needed, reacting to immediate tasks without waiting for user requests, similar to a junior employee making routine decisions.
- Level 3 agents proactively anticipate needs and evolve their own architecture, much like a seasoned professional preparing for future challenges.
- Self-building agents pose significant risks, such as creating inappropriate tools, compromising privacy, or making harmful decisions like favoring certain vendors over others.
- To mitigate risks, Foundation Capital suggests training agents gradually with low-risk tasks and human oversight, similar to how interns are guided in the workplace.
- Agents need safeguards like spending caps, multi-factor authentication, and validation of information sources to prevent errors and misuse.
- The future of AI agents depends on balancing capability with control, ensuring they generate ROI without replicating security vulnerabilities or creating bad code.
- Companies like Salesforce and Microsoft are already deploying AI agents, with Salesforce's Einstein GPT and Microsoft's Copilot leading the way.
- Enterprise adoption of agentic architectures is growing, with 12% of AI implementations now using them, and fewer companies relying on third-party vendors for gen AI tools.

## Key Topics Discussed

Alright, folks, let's dive into something that's really shaping up to be the next big thing in AI—self-building agents! In a world where AI is rapidly evolving from a tool we ask questions of to something that can act on its own, we're seeing a shift toward agentic systems. These are AI systems that take a goal, break it down into steps, execute those steps, and then combine the results to achieve the goal. Foundation Capital has been exploring this concept, and they're convinced that a System of Agents is going to revolutionize how we think about enterprise software. Essentially, it's a move from the old 'software-as-a-service' model to 'service-as-software'. Imagine AI agents systematically dismantling those complex enterprise software stacks we've all grown accustomed to over the past few decades!

Yohei Nakajima, the brilliant mind behind projects like BabyAGI and Pippin the Unicorn, has been talking about the unique challenges and opportunities that come with self-building agents. He outlined four levels of autonomy for these agents:

1. **Level 0**: These are your basic AI tools with fixed capabilities. Think of them as the Swiss Army knives of AI—they do what they're programmed to do, like ChatGPT with its predefined plugins or a Slack AI assistant that can summarize and write.

2. **Level 1**: Here, agents can create new tools, but only when explicitly asked by a user. It's like having an intern who can only build something new if you specifically request it. For example, if you need a parser for a specialized data format, you can ask the agent to create one. It’s reactive but still requires human input to expand its capabilities.

3. **Level 2**: These agents start building tools on their own when they detect a need. If you ask a question and the existing tools don’t cover it, the agent will automatically generate a new tool to handle the task. This is akin to a junior employee making routine decisions without needing constant supervision. Over time, the agent’s toolkit grows organically, but it’s still focused on immediate needs rather than future planning.

4. **Level 3**: This is where things get really interesting—and a bit concerning. Level 3 agents don’t just react to what you ask; they anticipate what you might need next and proactively evolve themselves. They can modify their own architecture or algorithms to stay ahead of changing requirements. For instance, if they expect data to start coming in via streaming APIs instead of static uploads, they’ll prepare for that in advance. It’s like having a seasoned professional on your team who’s always thinking ahead. But with great power comes great responsibility, right?

The risks here are substantial. A Level 3 agent could create tools that are powerful but inappropriate. Picture a corporate agent tasked with maximizing efficiency—it might start monitoring employee behavior, which could be a massive privacy violation. Or imagine an agent optimizing a supply chain, but in the process, it systematically cuts out smaller vendors, harming partnerships and reputation. Even more unsettling is the thought of a personal AI agent with access to your credit card, booking travel based on the cheapest or fastest options—only to fall prey to deceptive pricing schemes or manipulated search results. Yikes!

So, how do we prevent these kinds of mistakes? Yohei Nakajima’s answer is to develop self-building agents in a constrained, trust-building way—similar to how we train human talent. Start with low-risk tasks, like web scraping, to build the agent’s competence. As it proves reliable, gradually give it more complex responsibilities, like financial decision-making, but always keep human oversight and strict limitations in place.

We also need to equip these agents with safeguards. Think of it like teaching an intern to spot red flags—agents need to learn how to validate information sources, recognize potential scams, and balance priorities. For financial operations, agents should have spending caps and multi-factor authentication to prevent costly errors. Regular human feedback is another critical guardrail to ensure agents are learning the right behaviors.

Right now, we're on the brink of AI agents becoming everywhere—managing workflows, handling schedules, and tackling daily tasks. Salesforce has already rolled out Einstein GPT agents to its massive customer base of 150,000 companies, and Microsoft has introduced Copilot agents to 1.4 billion Windows users. It’s clear that enterprises are getting serious about agent-building. In fact, according to Menlo Ventures’ annual survey, agentic architectures were the biggest breakthrough in enterprise AI in 2024, powering 12% of implementations. And there’s a shift happening too: fewer companies are relying on third-party vendors for their gen AI tools, with around half now building their own internal solutions. This shows growing confidence in developing AI tools in-house.

Here’s the kicker: as agents become standard business tools, getting their self-building capabilities right isn’t just a technical challenge—it’s an organizational imperative. The balance between what agents can do and how much control we maintain over them will determine whether they generate real ROI or become a liability, replicating security vulnerabilities or churning out bad code. Just as we wouldn’t let a medical resident perform surgery unsupervised, we can’t release autonomous, self-building agents into the business world without proper training and guardrails.

The future of AI isn’t just about what these agents can do; it’s about how thoughtfully we teach them to teach themselves. We’re talking about a paradigm shift, and it’s going to be fascinating to watch how this plays out. If you're working on or interested in AI agents, Foundation Capital wants to hear from you—you can reach out to them at jchen@foundationcap.com.

