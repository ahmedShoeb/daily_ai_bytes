---
layout: post 
title: "Why are embeddings so cheap?"
blog_url: https://www.tensoreconomics.com/p/why-are-embeddings-so-cheap?utm_source=tldrai 
---



## Key Points

- Embeddings are fundamental for RAG systems and are significantly cheaper than generative models.
- The low cost of embeddings is due to the minuscule computational cost per token and the convergence of all embedding models to similar semantic representations, eliminating a sustainable moat for providers.
- The price to process a million input tokens for SOTA embedding models can be driven below 1¢ with sufficient demand and scale.
- Modern embedding models are primarily compute-bound, not memory-bound, meaning they don't significantly benefit from batching.
- FLOPS/dollar is the crucial hardware parameter for optimizing cost structure in building an embedding API.
- Generating an embedding requires a single forward pass through the model, with the compute footprint being similar to the prefill phase of autoregressive models.
- The majority of time during a forward pass for embeddings is spent in large-scale matrix multiplications.
- Real-world benchmarks show that increasing batch size for embeddings does not significantly improve throughput but severely impacts per-user latency due to compute-bound nature.
- GPUs like RTX 4090 offer better FLOPS/dollar ratios for embedding generation compared to H100, leading to lower costs per million tokens processed.
- The article concludes that the cheapness of embedding APIs is due to low underlying production costs and competitive pressure, hinting at "intelligence involution" for generative models too.

## Key Topics Discussed

The article "Why are embeddings so cheap?" by Tensor Economics delves into the economic and technical reasons behind the significantly lower cost of embedding APIs compared to generative AI models like GPT or Gemini. It argues that the low prices stem from the inherently minuscule computational cost of producing embedding representations and the lack of a sustainable competitive advantage among providers, as various embedding models converge to similar semantic outputs. The analysis highlights that state-of-the-art embedding models, such as Qwen3-Embedding-8B, are primarily compute-bound rather than memory-bound. This characteristic means that while a single forward pass is required for embedding generation, it involves large-scale matrix multiplications that fully saturate available compute resources. Consequently, increasing batch sizes, though seemingly beneficial for throughput, offers only marginal improvements in embeddings processed per second and severely increases latency for individual users. This contrasts with autoregressive text generation, which is memory-bound and significantly benefits from batching to amortize memory costs. The article estimates that with sufficient demand and scale, the cost of processing a million input tokens for leading embedding models can drop below 1¢. It further emphasizes that the critical hardware parameter for optimizing the cost structure of an embedding API is "FLOPS/dollar." Through benchmarking, it demonstrates that GPUs like the consumer-grade RTX 4090 can offer a superior FLOPS/dollar ratio for embedding workloads compared to more expensive professional GPUs like the H100, leading to even lower per-token processing costs. The author concludes that the efficiency of GPUs for these specific workloads, coupled with competitive pressures, drives embedding prices down, providing a glimpse into a future of "intelligence involution" for generative models as well.

