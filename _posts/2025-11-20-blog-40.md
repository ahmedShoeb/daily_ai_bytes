---
layout: post 
title: "Bits Per Sample"
blog_url: https://www.dwarkesh.com/p/bits-per-sample?utm_source=tldrai 
---



## Key Points

The article compares the learning efficiency of reinforcement learning (RL) and supervised learning (SL), focusing on "Bits/Sample."
In SL, every token provides a direct signal, leading to high information density per sample, especially early in training.
In RL, a single reward signal is received only after a long trajectory, and the information density per sample is much lower, particularly when the model is not yet smart.
The amount of new information learnable in SL is -log(p) bits/sample, where p is the pass rate, while in RL, it's at most Entropy(p).
RL has comparable information density to pre-training only in a tiny slice at the very end of training, when the model is already quite good.
Early in RL training, the gradient estimate is noisy due to low pass rates, making training difficult.
SL faces high variance at the end of training due to irreducible loss from unpredictable internet text.
RL works best in a "Goldilocks zone" of pass rates (where pass rate is >>1%), which can be achieved through pretraining, inference scaling, curriculum learning, and self-play.
While RL learns fewer "Bits/FLOP," these bits are highly valuable as they relate directly to performing economically valuable tasks and correcting mistakes.
The article suggests that RLVR might be eliciting latent capabilities rather than teaching de-novo strategies if the pretrained model doesn't have a high enough pass rate.
The "jaggedness" of RLVR might lead models to associate thought patterns to problem types rather than developing generalizable policies.
Human learning is highlighted as a more efficient process, not relying on binary outcomes but continuously updating a world model.

## Key Topics Discussed

The article, "Bits Per Sample," delves into the disparities in learning efficiency between reinforcement learning (RL) and supervised learning (SL), emphasizing the crucial metric of "Bits/Sample." It explains that while pretraining or supervised learning benefits from a direct signal on every token, providing high information density, RL faces a significant challenge. In RL, a single reward signal is only obtained after an entire thinking trajectory, which can be thousands of tokens long. Consequently, the information density per sample in RL is considerably lower, especially during the initial stages when the model is randomly initialized and unlikely to achieve correct outcomes.

The author mathematically illustrates this difference, noting that in supervised learning, the information gained corresponds to -log(p) bits/sample, where 'p' is the pass rate, indicating how surprised the model is by the correct answer. In contrast, for RL, the maximum information obtainable is bounded by Entropy(p) bits/sample, as it only provides a binary outcomeâ€”right or wrong. This comparison reveals that RL only achieves comparable information density to pretraining in a very narrow window at the very end of training, once the model has already become quite proficient.

Furthermore, the article discusses the issue of variance in RL. Early in training, with a low pass rate, the gradient estimates are highly noisy, hindering effective training. Conversely, supervised learning encounters high variance towards the end of training due to the irreducible loss inherent in unpredictable internet text.

To mitigate these challenges, the article proposes that RL functions optimally within a "Goldilocks zone" where the pass rate is significantly greater than 1%. Strategies like pretraining, inference scaling, curriculum learning, and self-play are identified as methods to achieve and maintain models within this optimal learning state. Self-play, for instance, helps maintain a 50% pass rate, maximizing information gain from binary outcomes.

Despite the lower "Bits/FLOP" in RL, the article acknowledges the high value of the bits learned. These bits are directly relevant to performing economically valuable tasks and enabling models to correct their mistakes, a capability often missing in pretraining. However, this valuable learning is restricted to a small fraction of the pass rate range.

The concept of "RLVR" (Reinforcement Learning from Human Feedback) is introduced, with the observation that it often elicits capabilities already latent in pretrained models rather than fostering de-novo strategies. The article also touches upon the "jaggedness" of RL, where models might learn to associate specific thought patterns with problem types instead of developing more generalizable problem-solving policies.

Finally, the discussion extends to human learning, positing it as a far more efficient process. Unlike model-free RL, which relies on binary outcomes at the end of an episode, human learning continuously updates a world model through observations and reflections, independent of the final outcome. This suggests a potential alternative path for AI training that moves beyond simple next-token prediction or outcome-based rewards, aiming to emulate how humans extract vast amounts of information from their environment.

