---
layout: post 
title: "Enhancing LLM Interaction with Structured Data: LlamaSheets and OpenAI Function Calling Fine-tuning"
blog_url: https://threadreaderapp.com/thread/1995543962342551738.html?utm_source=tldrai 
---



## Key Points

- Claude's current Excel integration is basic, lacking semantic understanding due to low-level coding.
- LlamaSheets API provides a solution by segmenting Excel sheets into structured tables, enabling LLMs to use Pandas/SQL and gain semantic awareness.
- A guide is available for integrating LlamaSheets with coding agents.
- OpenAI has introduced new support for function calling fine-tuning, enhancing GPT-3.5's ability to structure outputs and improve reasoning.
- LlamaIndex offers immediate support for function fine-tuning and GPT-4 distillation with Pydantic.
- Function fine-tuning is expected to advance agentic reasoning and RAG systems by allowing better structured outputs and data collection for training.
- Fine-tuned models demonstrate superior Chain of Thought reasoning and accuracy compared to base GPT-3.5, as shown in financial analysis examples.
- A method involves autogenerating questions over documents and logging agent interactions for fine-tuning.

## Key Topics Discussed

Hello podcast listeners! Today, we're diving into some fascinating advancements in how large language models interact with structured data, specifically Excel, and the power of function calling fine-tuning. It seems current approaches, like Claude's interaction with Excel, are quite rudimentary, relying on low-level Python code without truly understanding the data's semantic meaning. But here's where things get exciting! The new LlamaSheets API is a game-changer. It automatically segments complex Excel sheets into well-formatted 2D tables, instantly giving LLMs like Claude Code semantic awareness. This means they can now run sophisticated Pandas or SQL queries over properly structured dataframes, opening up a world of more advanced analysis. There's even a guide available to help you integrate LlamaSheets with your coding agents.

On another front, OpenAI has just rolled out new support for function calling fine-tuning, a development that's generating a lot of buzz! This feature is designed to help models like GPT-3.5 better structure their outputs, improve their reasoning, and enhance their planning capabilities. In a swift response, LlamaIndex has already released day-zero support for function fine-tuning, allowing for the distillation of GPT-4 with Pydantic. This innovative approach uses Pydantic schemas for OpenAI function calling, enabling the extraction of highly structured outputs. A significant benefit is the ability to log these results and compile datasets, which are crucial for further fine-tuning. The LlamaIndex team is particularly keen on leveraging function fine-tuning to explore more advanced agentic reasoning and to enhance Retrieval Augmented Generation, especially with its new support for structured outputs. What's truly impressive is the evidence showing that fine-tuned models outperform base GPT-3.5 in Chain of Thought reasoning, leading to more accurate answers, as demonstrated in examples involving financial asset valuation. The comprehensive guide even details how to achieve this, by automatically generating questions over financial filings and logging the prompt inputs and outputs of a GPT-4 agent to fine-tune GPT-3.5. It's clear that these developments are pushing the boundaries of what LLMs can do with structured information and complex reasoning!

