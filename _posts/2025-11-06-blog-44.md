---
layout: post 
title: "AWS and OpenAI Announce Multi-Year Strategic Partnership for Advanced AI Workloads"
blog_url: https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure?utm_source=tldrai 
---



## Key Points

OpenAI has entered a multi-year, strategic partnership with AWS to gain immediate and increasing access to AWS's world-class infrastructure for advanced AI workloads.
AWS will provide OpenAI with Amazon EC2 UltraServers, featuring hundreds of thousands of chips, and the capacity to scale to tens of millions of CPUs for its generative AI needs.
The agreement represents a $38 billion commitment, enabling OpenAI to rapidly expand its compute capacity while leveraging AWS's price, performance, scale, and security.

## Key Topics Discussed

Hey podcast listeners! We've got some big news in the AI world. Amazon Web Services, or AWS, and OpenAI have just announced a major multi-year strategic partnership. This means OpenAI is getting immediate and growing access to AWS's top-tier infrastructure to power their advanced artificial intelligence workloads. We're talking about a massive $38 billion agreement over the next seven years! OpenAI will be tapping into AWS compute, which includes hundreds of thousands of state-of-the-art NVIDIA GPUs, with the potential to expand to tens of millions of CPUs for their agentic workloads. AWS is known for securely and reliably running large-scale AI infrastructure, some with clusters exceeding 500,000 chips. This collaboration leverages AWS's cloud infrastructure leadership with OpenAI's groundbreaking work in generative AI to enhance user experience with ChatGPT. The demand for computing power in AI is soaring, and frontier model providers are increasingly turning to AWS for its performance, scale, and security. OpenAI is set to start utilizing this AWS compute immediately, with all capacity planned for deployment by the end of 2026, and further expansion possible into 2027 and beyond. The infrastructure AWS is building for OpenAI features a sophisticated design, optimized for maximum AI processing efficiency and performance. By clustering NVIDIA GPUs, both GB200s and GB300s, via Amazon EC2 UltraServers on the same network, they achieve low-latency performance across interconnected systems. This setup allows OpenAI to run diverse workloads, from serving ChatGPT inference to training next-generation models, with the flexibility to adapt as their needs evolve. Sam Altman, OpenAI's co-founder and CEO, emphasized that scaling frontier AI demands massive, reliable compute, and this partnership strengthens the broad compute ecosystem for the next era of advanced AI for everyone. Matt Garman, CEO of AWS, added that AWS's best-in-class infrastructure will be a backbone for OpenAI's AI ambitions, showcasing AWS's unique ability to support vast AI workloads with immediate availability of optimized compute. This partnership isn't entirely new; the companies have been working together for a while to bring cutting-edge AI to organizations globally. Earlier this year, OpenAI's open-weight foundation models became available on Amazon Bedrock, giving millions of AWS customers access to these additional model options. OpenAI has quickly become one of the most popular model providers on Amazon Bedrock, with thousands of customers using their models for various applications like agentic workflows, coding, scientific analysis, and mathematical problem-solving.

