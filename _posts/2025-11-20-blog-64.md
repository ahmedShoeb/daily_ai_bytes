---
layout: post 
title: "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI"
blog_url: https://arxiv.org/abs/2511.07885?utm_source=tldrai 
---



## Key Points

- Centralized cloud infrastructure for LLM queries is strained by growing demand.
- Small LMs are achieving competitive performance, and local accelerators can run them efficiently.
- The paper introduces "intelligence per watt (IPW)" as a metric for local inference capability and efficiency.
- A study was conducted across 20+ local LMs, 8 accelerators, and 1M real-world queries.
- Local LMs can accurately answer 88.7% of single-turn chat and reasoning queries, with domain-specific variations.
- From 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%.
- Local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models.
- These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure.
- The IPW profiling harness has been released for systematic intelligence-per-watt benchmarking.

## Key Topics Discussed

Hey podcast listeners! We're diving into a fascinating new paper titled "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI." This research tackles a growing problem: the immense strain on centralized cloud infrastructure from the ever-increasing demand for Large Language Model, or LLM, queries. But here's the exciting part â€“ the authors propose a shift towards local AI inference, powered by smaller, yet powerful, LLMs and efficient local accelerators. 

The core of their work introduces a brilliant new metric called "intelligence per watt," or IPW. Think of it as a way to measure both the capability and efficiency of local inference across different model and accelerator combinations. To put this to the test, they conducted a massive empirical study. They looked at over 20 state-of-the-art local LLMs, 8 different accelerators, and a whopping 1 million real-world chat and reasoning queries. They measured everything: accuracy, energy consumption, latency, and power. 

And the results are really compelling! First off, local LLMs proved quite capable, accurately answering 88.7% of single-turn chat and reasoning queries, though there were some variations depending on the domain. Even more impressive, the efficiency of local AI has seen a dramatic improvement. From 2023 to 2025, IPW jumped by 5.3 times, and the coverage of queries that could be handled locally soared from 23.2% to 71.3%. 

But here's the kicker: local accelerators demonstrated at least 1.4 times lower IPW compared to cloud accelerators running the exact same models! This highlights a huge potential for optimization in local AI. These findings strongly suggest that local inference isn't just a pipe dream; it can genuinely help redistribute the demand currently bottlenecking centralized cloud infrastructure. And to help the community keep track of this exciting transition, the researchers have even released their IPW profiling harness for systematic benchmarking. This is a big step towards more efficient and distributed AI!

