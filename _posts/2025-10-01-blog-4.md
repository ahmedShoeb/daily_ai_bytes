---
layout: post 
title: "We reverse-engineered Flash Attention 4"
blog_url: https://modal.com/blog/reverse-engineer-flash-attention-4?utm_source=tldrai 
---



## Key Points

- Flash Attention 4 (FA4) is the latest generation of CUDA kernels for Transformer neural networks, optimized for Nvidiaâ€™s Blackwell Streaming Multiprocessor architecture.
- FA4 achieves a reported ~20% speedup over previous state-of-the-art attention kernels.
- The article reverse-engineers the FA4 kernel, highlighting its complex asynchronous pipeline and specific mathematical optimizations.
- A key innovation in FA4 is its asynchronous 
pipeline of operations, managed through 
warp specialization, where 32-thread groups (warps) handle different pipeline steps.
- New features include faster approximate exponentials using CUDA Cores to avoid SFU bottlenecks and a more efficient online softmax that intelligently updates the normalization scaling factor, reducing corrections by a factor of 10.
- The kernel's operation involves five specialized warps: Load, MMA, Softmax, Correction, and Epilogue, each responsible for distinct stages of data processing and memory management.

## Key Topics Discussed

The article details the reverse-engineering of Flash Attention 4 (FA4), a cutting-edge CUDA kernel designed for Transformer neural networks and optimized for Nvidia's Blackwell Streaming Multiprocessor architecture. Flash Attention kernels are crucial for generative AI workloads, addressing the primary bottlenecks in attention layers. FA4 reportedly achieves a ~20% speedup compared to previous methods, including Nvidia's cudnn library kernels, and its source code was publicly released. The authors explain that FA4's most significant innovation lies not just in its mathematical tricks, but in a substantial increase in the complexity of its asynchronous 'pipeline' of operations, making it understandable to software engineers familiar with parallel and concurrent programming. The write-up is divided into a 'quick tour' for general software engineers and a 'deep dive' for GPU specialists. The 'quick tour' introduces the 'Life of a Tile,' tracing how a block of inputs is transformed into outputs. The process involves splitting large input tensors into smaller 'tiles' and processing them concurrently. A key aspect is 'warp specialization,' where chunks of the pipeline are mapped onto 32-thread warps, enabling asynchronous execution and efficient resource utilization. The kernel's pipeline follows a producer/consumer model with manual synchronization using barriers. High-level steps include loading query tiles into shared memory, streaming key and value tiles, multiplying keys with queries using Tensor Cores to produce unnormalized attention scores, and then processing these scores through Softmax and Correction warps. Significant new features in FA4 include faster approximate exponentials using CUDA Cores instead of Special Function Units (SFUs) for the exponential step in normalization, particularly for smaller attention head sizes. This involves a cubic polynomial approximation to compute 2 ** x, avoiding SFU bottlenecks. Additionally, a more efficient online softmax is implemented where the normalization scaling factor is updated much more intelligently. Instead of updating every time a new maximum is observed, updates are applied only when the maximum changes enough to impact numerical stability, reducing costly output rescaling operations by a factor of 10. The 'deep dive' elaborates on the roles of the five specialized warps: Load warp, MMA warp, Softmax warps, Correction warps, and Epilogue warp(s). Each warp is responsible for distinct stages of data processing and memory management, from loading data using the Tensor Memory Accelerator (TMA) to computing attention scores, normalizing them, correcting past outputs, and finally storing results back into global memory. The article concludes by reflecting on the evolution of GPU programming, noting the increasing reliance on programmer-managed asynchrony and the growing complexity, while also highlighting the exciting advancements in high-performance numerical computing.

