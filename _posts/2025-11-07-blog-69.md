---
layout: post 
title: "Run Any LLM with a Single API: Introducing any-llm v1.0"
blog_url: https://blog.mozilla.ai/run-any-llm-with-a-single-api-introducing-any-llm-v1-0/?utm_source=tldrai 
---



## Key Points

any-llm v1.0 allows running various LLMs (OpenAI, Claude, Mistral, llama.cpp) from a single interface.
It offers production-ready stability, standardized reasoning output, and auto-provider detection.
It simplifies switching between cloud and local model providers, reducing boilerplate code and integration issues.
Version 1.0 brings improved test coverage, Responses API support, List Models API, re-usable client connections, and standardized reasoning output.
The guiding principle is simplicity, with a stable interface while providers evolve behind the scenes.
any-llm v1.0 is built for real workloads with async-first APIs and clear deprecation notices.
Future plans include native batch completions, support for new providers, and deeper integrations with other any-suite libraries.

## Key Topics Discussed

Hey everyone! We've got some exciting news from Mozilla.ai with the launch of any-llm v1.0. This is a game-changer for anyone working with large language models, as it provides a single, unified interface to run a variety of LLMs, whether they're cloud-based like OpenAI and Claude, or local models such as Mistral and llama.cpp. This really embodies Mozilla.ai's mission to make AI more transparent, interoperable, and accessible. In today's fast-paced LLM landscape, with new models and APIs popping up constantly, any-llm aims to free developers from vendor lock-in and the hassle of rewriting their code stack every time they want to try a new model. This means less boilerplate and more flexibility for you! Since its initial release, any-llm has seen significant improvements in version 1.0, focusing on stability and enhanced features. We're talking improved test coverage, support for the Responses API, a List Models API to query supported models programmatically, and even re-usable client connections for better performance. A standout feature is the standardized reasoning output across all models, ensuring you get consistent results no matter which provider you choose. They've even got an auto-updating provider compatibility matrix, which is super helpful. Mozilla.ai's guiding principle here is simplicity â€“ a stable interface that lets the individual providers evolve their tech behind the scenes. This isn't just for prototyping; any-llm v1.0 is built for real production workloads. It boasts a stable and consistent API surface, async-first APIs, and re-usable client connections, perfect for high-throughput and streaming applications. Plus, they're upfront with clear deprecation and experimental notices, so no surprises there. By decoupling your product logic from specific model providers, any-llm offers incredible flexibility for scaling your projects. And the journey doesn't stop here! Mozilla.ai has big plans for the future, including native batch completions, support for even more providers, and deeper integrations with their other 'any-suite' libraries like any-guardrail, any-agent, and mcpd. They're also really keen to hear your feedback, so if you're using any-llm, jump into their Github or Discord to share your thoughts and help make it even better!

