---
layout: post 
title: "Understanding Programming Language Concepts and Advances in Self-Supervised Audio Learning"
blog_url: https://threadreaderapp.com/thread/1974625322609049803.html?utm_source=tldrai 
---



## Key Points

New programming languages are challenging due to unfamiliar syntax and complex semantics.
Syntax defines textual well-formedness, including symbols, formatting, operators, and keywords.
Semantics define program behavior, encompassing runtime and compile-time aspects like type checking.
New research introduces self-supervised learning for audio using differentiable ranking.
This method extends permutation pre-text tasks and improves performance on low-resource audio tasks, as well as images and video.
The approach overcomes limitations of traditional permutation pretraining by allowing arbitrary permutations through a differentiable ranking objective.
Increased usable permutations lead to better learned representations for various downstream tasks.

## Key Topics Discussed

This article discusses two distinct but important topics: the fundamental concepts of programming languages and a novel advancement in self-supervised learning for audio. In the context of programming languages, it highlights that learning new languages is often difficult due to both unfamiliar syntax and challenging semantics. Syntax refers to the textual well-formedness of a program, encompassing elements like symbols, formatting, whitespace, operators, and keywords. Semantics, on the other hand, define the program's behavior, including runtime execution and compile-time aspects such as type checking, which is often the real challenge. The article uses examples like APL versus Python to illustrate vast differences in syntax. Separately, the content introduces new work on self-supervised learning for audio. This research extends the permutation pre-text task by incorporating a differentiable ranking objective. This innovation allows for the use of arbitrary permutations, addressing the limitation in traditional permutation pretraining where only a small subset of n! permutations could be effectively used as classes for a classifier. By increasing the number of usable permutations, the method demonstrates improved learned representations, which in turn leads to enhanced performance on low-resource audio tasks, and is also effective for images and video.

