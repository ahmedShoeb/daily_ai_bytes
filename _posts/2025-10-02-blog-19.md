---
layout: post 
title: "Inside NVIDIA GPUs: Anatomy of high performance matmul kernels"
blog_url: https://www.aleksagordic.com/blog/matmul?utm_source=tldrai 
---



## Key Points

- The article provides an in-depth look into the design of high-performance matrix multiplication (matmul) kernels on NVIDIA GPUs, focusing on the Hopper architecture.
- It covers GPU hardware architecture, including memory hierarchy (GMEM, L2, SMEM, L1, Registers) and compute units (SMs, Tensor Cores).
- The CUDA programming model and its mapping to GPU hardware are explained, emphasizing concepts like threads, warps, and blocks, along with performance-critical aspects like GMEM coalescing and SMEM bank conflicts.
- NVIDIA's assembly languages, PTX and SASS, are introduced as tools for fine-tuning performance, highlighting their role in achieving the last few percent of optimization.
- A near-SOTA synchronous matmul kernel using the warp-tiling method is detailed, focusing on arithmetic intensity and data reuse without relying on advanced Hopper features.
- Modern Hopper GPU optimizations are explored, leveraging Tensor Memory Accelerator (TMA), Tensor Cores, and bf16 precision for asynchronous data movement and computation.
- Data swizzling is explained as a technique implemented by TMA to optimize memory access patterns and reduce bank conflicts.
- The use of `wgmma.mma_async` instructions for warp-group cooperative asynchronous matrix multiplication is highlighted as a powerful Hopper feature.
- Advanced techniques like pipelining compute and data movement using SMEM barriers, persistent kernels, Hilbert-curve scheduling, and cluster-level execution with DSMEM are presented for further performance gains.
- Various micro-optimizations and a performance table quantify the cumulative impact of these detailed engineering efforts, demonstrating significant improvements from baseline to SOTA.

## Key Topics Discussed

This extensive blog post by Aleksa Gordic delves into the intricate details of designing high-performance matrix multiplication (matmul) kernels for NVIDIA GPUs, particularly focusing on the Hopper architecture. The article is structured into four main parts, beginning with the fundamental concepts of NVIDIA GPU architecture. It meticulously explains the hierarchical memory system, comprising device memory (VRAM), L2 cache, distributed shared memory (DSMEM), L1 cache, shared memory (SMEM), and the register file, highlighting the trade-offs between speed, capacity, and latency at each level. The compute components, such as Streaming Multiprocessors (SMs), Tensor Cores, CUDA cores, and warp schedulers, are also thoroughly described, emphasizing their roles in parallel computation. The author then transitions to the CUDA programming model, detailing abstractions like threads, warps, thread blocks, and grids, and how they map onto the underlying hardware. Crucial performance considerations, such as GMEM coalescing and avoiding SMEM bank conflicts, are elaborated with practical examples demonstrating their impact on throughput. The discussion moves to GPU assembly languages, PTX and SASS, explaining their importance for achieving peak performance by understanding and guiding the compiler's output. A significant portion of the article is dedicated to designing matmul kernels. It first presents a "near-SOTA" synchronous matmul kernel built around the warp-tiling method, which optimizes arithmetic intensity and data reuse without relying on advanced Hopper features. This section illustrates how understanding loop permutations and breaking computations into partial sums of outer products can dramatically improve efficiency. The latter part of the article focuses on achieving "true SOTA" asynchronous matmul kernels on Hopper GPUs. Here, advanced hardware features like the Tensor Memory Accelerator (TMA), Tensor Cores, and bf16 precision are introduced. TMA is shown to simplify asynchronous data transfers and automatically handle data swizzling, a technique explained in detail to optimize memory access patterns and eliminate bank conflicts. The `wgmma.mma_async` instruction, enabling warp-group cooperative asynchronous matrix multiplication, is also a key component. Further optimizations include pipelining compute and data movement using SMEM barriers and a producer-consumer model with warp-groups. Concepts like persistent kernels, cache-aware scheduling (e.g., Hilbert curves), and leveraging Hopper's cluster-level execution model with DSMEM for inter-SM communication are also covered to reduce L2/GMEM traffic and enhance locality. The article concludes by listing various micro-optimizations and presenting a performance table that quantifies the gains achieved at each stage of optimization, underscoring the cumulative impact of these detailed engineering efforts. The author emphasizes the belief that complex computer systems can be understood through systematic dissection.

