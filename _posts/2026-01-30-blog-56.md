---
layout: post 
title: "Google and Stanford Researchers Build AI ‘Generative Agents’ That Mimic Real People After a Two‑Hour Interview"
blog_url: https://gizmodo.com/google-researchers-can-create-an-ai-that-thinks-a-lot-like-you-after-just-a-two-hour-interview-2000547704 
---



## Key Points

- Stanford and DeepMind teamed up to interview 1,052 U.S. adults for two hours each, then used the transcripts to create AI “generative agents” that imitate each participant.
- The AI agents matched the real people’s answers on standard surveys (General Social Survey) about 85% of the time and on personality tests (Big Five) about 80% of the time.
- When the agents played economic games like the Prisoner’s Dilemma and the Dictator’s Game, their behavior diverged, achieving only roughly a 60% correlation with the humans.
- Researchers envision these agents as a new kind of focus‑group tool for policymakers, marketers, and social scientists, allowing them to test reactions to policies or product launches at scale.
- The study raises ethical red flags: if AI can convincingly simulate a person’s opinions, the technology could be weaponized for manipulation, targeted persuasion, or even fraud.
- The interview platform used a quirky 2‑D sprite avatar and began with participants reading the first two lines of *The Great Gatsby* to calibrate audio.
- The entire workflow—from recruitment via a market‑research firm to feeding the transcript into a large language model—was largely automated.

## Key Topics Discussed

Hey listeners, you’re about to hear a wild story from the frontier of AI and social science. Researchers at Stanford, in partnership with Google’s DeepMind, decided to see just how well a large language model could impersonate a real human after a single, two‑hour chat. They paid a thousand‑plus volunteers $60 each, had them read the opening lines of *The Great Gatsby* to set the tone, and then let a cute 2‑D sprite interview them about everything from politics and race to their daily stressors. Those conversations averaged about 6,500 words per person. 

All that data was fed into an LLM, which then spun up a “generative agent” for each participant. The scientists tested the clones against the originals using the General Social Survey and the Big Five personality inventory, and the results were surprisingly high—about 85% agreement on survey answers and 80% on personality traits. 

But the magic faded when the agents were thrown into classic economic games like the Prisoner’s Dilemma and the Dictator’s Game. Here, the AI’s decisions only lined up with the humans about 60% of the time, showing that while the models can mimic surface‑level opinions, they struggle with the nuanced, strategic thinking that real people bring to high‑stakes scenarios. 

Why does this matter? The researchers pitch these agents as a next‑gen focus‑group: imagine asking a thousand simulated citizens how they’d react to a new health policy or a product launch, without ever having to poll the real crowd again. It sounds efficient, but it also opens a Pandora’s box. If corporations or politicians can rely on AI‑generated approximations of public sentiment, they might sidestep genuine democratic engagement. Worse, malicious actors could weaponize these personas to scam or manipulate people, much like the recent scams where AI‑powered “relatives” coaxed seniors into handing over bank details. 

In short, the study proves we can compress a person’s self‑report into a language model that behaves convincingly most of the time, but it also warns us that the technology is still brittle in high‑risk contexts—and that the ethical stakes are huge. As we keep pushing the boundaries of AI, we’ll need to ask ourselves: just because we can simulate a human, should we? Stay tuned for more deep dives into the tech shaping our world.

