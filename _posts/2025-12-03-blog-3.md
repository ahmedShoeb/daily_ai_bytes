---
layout: post 
title: "How prompt caching works - Paged Attention and Automatic Prefix Caching plus practical tips"
blog_url: https://sankalp.bearblog.dev/how-prompt-caching-works/?utm_source=tldrai 
---



## Key Points

Prompt caching reuses computed key-value tensors for identical prompt prefixes, saving costs and reducing latency in LLM inference.
Practical tips to improve cache hits include maintaining a stable prefix, using append-only context, employing deterministic serialization for tool outputs, and avoiding dynamic changes to tool definitions.
LLM inference involves prefill (initial input processing) and decode (token generation), with KV caching optimizing the latter by storing and reusing KV tensors.
Traditional KV cache allocation faces memory challenges like internal/external fragmentation and redundancy for shared prefixes.
Paged Attention (used in vLLM) solves these by allocating fixed-size blocks for KV tensors, similar to OS paging.
Block hashing with parent chaining creates content-addressable blocks, ensuring that if a block's hash matches, its entire prefix is identical.
A hashmap facilitates O(1) lookup for cached blocks, enabling efficient reuse across different requests.
Prefix caching leverages these mechanisms to identify and skip prefill computation for cached blocks, significantly optimizing LLM inference.
Caching works at the token level and is content-based, allowing different users to benefit from shared cached blocks if their prompts have common prefixes.
A static and unchanging prefix is crucial for effective prompt caching due to the causal attention mechanism and the hash chain integrity.

## Key Topics Discussed

Hey there, podcast listeners! Today, we're diving deep into a super interesting topic that's making our interactions with AI models faster and cheaper: **Prompt Caching**. Our article of the day, "How prompt caching works - Paged Attention and Automatic Prefix Caching plus practical tips," breaks down the magic behind it.

Basically, prompt caching is all about efficiency. When you send a prompt to an LLM, part of that prompt might have been seen before. Instead of re-calculating everything from scratch, prompt caching allows LLM providers to reuse previously computed data, specifically those 'key-value' (KV) tensors. This means you get faster responses and, often, a lower bill â€“ who doesn't love that?

The article gives us some fantastic, practical tips to maximize these cache hits. The golden rule is to keep your prompt prefix as **stable** as possible. Think about making your system prompts universal and user-agnostic. Also, keep your context **append-only**; don't go truncating or changing things in the middle of a conversation, as this breaks the cache. If you're using tool outputs, ensure **deterministic serialization** (like sorting JSON keys) so the system recognizes the content as identical. And a big one: avoid dynamically changing your tool definitions, as they usually sit at the start or end of your system prompt and any alteration will invalidate the cache for subsequent tokens.

To truly understand prompt caching, we need a quick peek under the hood of LLM inference. There are two main stages: **prefill**, where the entire input prompt is processed to get the first token, and **decode**, where the model generates subsequent tokens one by one. Traditional KV caching helps during decoding by storing KV tensors, so the model doesn't recompute them for every token. However, this traditional method has its downsides, leading to **memory fragmentation** and **redundancy** when the same prefixes are computed repeatedly.

This is where innovative solutions like **Paged Attention**, used in the vLLM inference engine, come into play. Inspired by operating system paging, Paged Attention breaks down the KV cache into fixed-size blocks. These blocks are managed using a system where each block has a unique hash, which is chained to the hash of its parent block. This parent chaining is super smart because if a block's hash matches, it guarantees that all the preceding blocks in that prefix are also identical. This allows for an **O(1) lookup** using a hashmap, making block reuse incredibly efficient across different user requests.

So, when a new request comes in, the system quickly checks for the **longest cached prefix**. If parts of the prompt already exist in the cache, the engine skips the 'prefill' computation for those blocks and simply points to the existing KV tensors. This is the core of how prompt caching delivers those sweet cost and latency savings. It's not about caching per conversation; it's about caching per **content**, working at the token level, which means different users can benefit from each other's cached prefixes if they happen to share the same initial prompt content.

So, remember, keeping your prefixes static and consistent is key to harnessing the power of prompt caching. It's a fundamental optimization that makes large language models more efficient and accessible for everyone. Thanks for tuning in, and we'll catch you next time!

