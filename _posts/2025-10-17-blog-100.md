---
layout: post 
title: "Diffusion Transformers with Representation Autoencoders"
blog_url: https://rae-dit.github.io/?utm_source=tldrai 
---



## Key Points

Representation Autoencoders (RAEs) use frozen representation encoders and lightweight decoders for high-fidelity latents in Diffusion Transformers (DiT).
RAEs achieve faster convergence and better reconstruction quality (rFID) compared to SD-VAE.
The reconstruction quality of RAEs is stable across various encoder sizes and improves with increased decoder capacity.
Standard DiT training fails with RAE latents; the DiT model's width must match or exceed the RAE's token dimension for success.
Effective RAE-DiT training requires matching DiT width, a dimension-dependent noise schedule shift, and decoder noise augmentation.
The paper introduces DiTDH, a shallow-but-wide diffusion transformer head, to efficiently increase model width without quadratic compute.
DiTDH demonstrates substantial FLOP-efficiency and faster convergence than standard DiT.
RAE-based DiTDH-XL achieves new state-of-the-art FID scores on ImageNet at 256x256 (1.51 without guidance, 1.13 with guidance) and 512x512 (1.13 with guidance).
The structured representation of RAEs is crucial for strong performance, not merely high dimensionality.

## Key Topics Discussed

The article introduces Representation Autoencoders (RAEs) as an innovative approach to improve latent generative modeling within Diffusion Transformers (DiT). RAEs leverage pretrained, frozen representation encoders in conjunction with lightweight, trained decoders to generate high-fidelity and semantically rich latent representations. The research challenges the conventional belief that such pretrained encoders are unsuitable for reconstruction, demonstrating that RAEs consistently achieve superior reconstruction quality (rFID) compared to the standard SD-VAE. This quality is maintained across different encoder sizes and further enhanced by increasing decoder capacity. A significant finding is that the standard diffusion training recipe does not inherently work with RAEs. Successful generation necessitates that the DiT model's width matches or exceeds the RAE's token dimension. To address this, the authors propose three critical components for stable and efficient training: aligning DiT width with the encoder's token dimensionality, implementing a dimension-dependent shift in the noise schedule, and incorporating decoder noise augmentation. Furthermore, the paper introduces DiTDH (Diffusion Transformer with a Wide Head), a novel architecture that utilizes a shallow-but-wide diffusion transformer head. This design efficiently increases the model's width without incurring the quadratic computational costs associated with scaling the entire backbone. Empirically, the RAE-based DiTDH-XL model sets new state-of-the-art FID (Fr√©chet Inception Distance) scores on ImageNet, achieving 1.51 at 256x256 without guidance and 1.13 at both 256x256 and 512x512 with guidance, significantly outperforming previous diffusion models. The study emphasizes that the structured representation provided by RAEs is vital for achieving these performance gains, indicating that high dimensionality alone is insufficient. The authors conclude that RAE latents, coupled with the proposed training strategies and the DiTDH architecture, offer a robust and efficient foundation for future research in generative modeling with diffusion transformers.

