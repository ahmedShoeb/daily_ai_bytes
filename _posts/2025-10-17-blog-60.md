---
layout: post 
title: "LLM Evolution: Character Manipulation"
blog_url: https://blog.burkert.me/posts/llm_evolution_character_manipulation/?utm_source=tldrai 
---



## Key Points

Newer generations of LLMs (e.g., GPT-5, Claude 4.5) show improved capabilities in character manipulation, counting, and solving encoding/ciphers.
Earlier LLMs struggled with character-level tasks due to their tokenization process.
GPT 4.1 and Claude Sonnet 4 were the first models to consistently perform character replacement accurately.
GPT-4.1 reliably counted characters without reasoning, while GPT-5 models and Claude Sonnet models achieved this with reasoning.
State-of-the-art models now demonstrate a working understanding of Base64 decoding, even for out-of-distribution text, suggesting algorithmic comprehension beyond memorization.
LLMs are becoming more adept at character-level text manipulation, including substitution ciphers, despite their token-based understanding.

## Key Topics Discussed

The article explores the advancements of large language models (LLMs) in handling intricate natural language tasks, specifically character manipulation, counting, and solving encoding and ciphers. The author's tests revealed a significant improvement in the newest generations of LLMs, such as GPT-5 and Claude 4.5, compared to their predecessors. Historically, LLMs struggled with character-level operations due to text tokenization, which typically processes text in clusters of characters or whole words. However, models like GPT 4.1 and Claude Sonnet 4 have shown consistent success in tasks like character replacement. When it came to character counting, a known challenge for LLMs, GPT-4.1 stood out as the only model capable of reliably performing this task without relying on reasoning. With reasoning enabled, GPT-5 models across various sizes and Claude Sonnet models also successfully completed character counting. Further experimentation involved Base64 encoding and the ROT20 cipher. While older models frequently failed, particularly with unfamiliar text patterns, cutting-edge models, including GPT-5 and Gemini-2.5-pro, demonstrated a robust understanding of Base64 decoding. This suggests that these models have developed a genuine algorithmic comprehension rather than merely memorizing common English word patterns. The article concludes by highlighting that LLMs are increasingly proficient in character-level text manipulation, even with substitution ciphers, despite their fundamental token-based understanding of language. Although character-level operations are still an evolving area, the progress observed in these models is both remarkable and compelling.

