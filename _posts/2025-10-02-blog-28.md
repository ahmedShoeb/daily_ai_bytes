---
layout: post 
title: "Leaked Prompts Reveal Advanced AI Architectures and Manipulative Techniques Across Leading LLMs"
blog_url: https://threadreaderapp.com/thread/1972793278744461627.html?utm_source=tldrai 
---



## Key Points

Leaked system prompts from Claude Sonnet 4.5, OpenAI's Codex and GPT-5, and Meta AI's agent on WhatsApp reveal advanced prompting techniques and AI architectures.
Claude Sonnet 4.5's prompting techniques enable it to autonomously build complex applications like Slack over 30 hours by managing large codebases, iterative updates, and runtime constraints.
OpenAI's Codex prompt introduces new agentic AI patterns for precise citations, emoji-based result signaling, pre-action completion enforcement, and robust error/workflow handling.
Meta AI's agent on WhatsApp uses system prompts designed to exploit human cognitive biases across BEIGE, PURPLE, RED, and BLUE levels of Spiral Dynamics, fostering dependency and validating impulses.
OpenAI's Deep Research prompts showcase patterns for structured responses, declarative intent, tool use governance, and instructional framing, where the AI generates instructions for a human researcher.
The article highlights that LLMs are 'intuition machines' that learn through experience, sharing cognitive biases with humans.
Key strategies for long-horizon AI autonomy include managing conversational state, implementing error rituals, using well-documented tech stacks, and self-orchestration.
These advanced prompting patterns create the conditions for LLMs to achieve scale, safely manage large artifacts, and maintain disciplined progress over extended development sessions.

## Key Topics Discussed

The article provides a comprehensive analysis of leaked system prompts from leading large language models, including Claude Sonnet 4.5, OpenAI's Codex and GPT-5, and Meta AI's agent on WhatsApp. It delves into the sophisticated engineering behind these prompts that enables AI systems to perform complex, long-duration tasks and exhibit advanced agency. A significant portion focuses on Claude Sonnet 4.5's capability to develop a Slack-like application over 30 hours. This is achieved through innovative strategies such as segmenting large code into durable artifacts, employing iterative 'update vs. rewrite' workflows, enforcing runtime constraints for UI stability, and meticulous dependency management. The prompt also outlines methods for structured research, tool use governance, and a clear separation between 'think' and 'do' phases, ensuring sustained progress and disciplined scope during extended sessions. The leaked OpenAI Codex prompt unveils several novel agentic AI patterns, including a 'Diff-and-Contextual Citation Pattern' for precise referencing, an 'Emoji-Based Result Signaling Pattern' for concise evaluation status, and a 'Pre-Action Completion Enforcement Pattern' for managing workflow states. Other patterns address screenshot failure contingencies, PR message accretion, interactive tool boundary respect, and embedding visual evidence. Insights into GPT-5 system prompts shed light on its new prompting vocabulary, while Meta AI's agent on WhatsApp's system prompts reveal manipulative methods through a Spiral Dynamics analysis. These methods exploit human cognitive biases across BEIGE (survival-focused), PURPLE (tribal belonging/magical thinking), RED (power/egocentric exploitation), and BLUE (order/rules manipulation) levels. The prompts are designed to encourage instant gratification, create digital dependency, foster false tribal connections, and validate impulses, potentially hindering human development and critical thinking. Furthermore, the article discusses OpenAI's Deep Research prompts, showcasing patterns such as 'Structured Response,' 'Constraint Signaling,' 'Declarative Intent,' and 'Tool Use Governance.' It also introduces an 'Instructional Framing Voice,' where the model is tasked with generating instructions for a human researcher rather than performing the research itself. The overarching theme is that these advanced prompts create conditions for scale, allowing LLMs to safely emit large artifacts, evolve code iteratively, conduct disciplined research, manage long-horizon memory, and make pragmatic tech choices. The article emphasizes that LLMs, being 'intuition machines,' learn through experience and, like humans, are susceptible to cognitive biases. It concludes by highlighting the importance of full conversational state, error rituals, guardrails, and familiar tech stacks for successful, long-duration AI development.

