---
layout: post 
title: "Hybrid Models as First-Class Citizens in vLLM"
blog_url: https://pytorch.org/blog/hybrid-models-as-first-class-citizens-in-vllm/?utm_source=tldrai 
---



## Key Points

Large language models are facing scaling limits with attention mechanisms for long sequences due to KV cache growth and prefill latency.
Hybrid models, combining attention with Mamba or linear attention, offer a solution for efficient long-sequence inference while maintaining modeling quality.
vLLM V1 has elevated hybrid models to fully supported first-class citizens, moving beyond experimental status in V0.
Long sequences are crucial for real-world AI applications like Retrieval-Augmented Generation (RAG), agentic patterns, and complex reasoning.
State space models (SSMs) like Mamba provide linear scaling and fixed latent state dimensionality, addressing attention's bottlenecks.
vLLM V1 introduces a unified allocator for both KV cache and Mamba state, replacing the less efficient separate allocation in V0.
Adaptation for Mamba in V1 involved aligning attention block sizes with Mamba's larger page size and adjusting state tensor strides for compatibility.
Performance engineering in V1 included refactoring code and optimizing Triton kernels with CUDA Graphs, significantly improving throughput.
Benchmarks demonstrate vLLM V1 generally outperforms V0, with throughput gains up to 91% for some models using `FULL_AND_PIECEWISE` CUDA Graphs.
Hybrid models in vLLM V1 are now practical tools for building enterprise-ready AI systems, capable of handling large-scale, long-context AI workloads.

## Key Topics Discussed

Large language models are encountering significant scaling challenges with traditional attention mechanisms, particularly concerning the linear growth of the KV cache and quadratic increase in prefill latency for long sequences. To address these limitations, hybrid models, which blend attention with alternative architectures like Mamba or linear attention, are emerging as a promising solution. These hybrid approaches maintain modeling quality while enabling more efficient inference for extended contexts. The vLLM community has made substantial progress in integrating hybrid models, transitioning them from experimental "hacks" in vLLM V0 to fully supported "first-class citizens" in vLLM V1. This transformation is critical because long sequences are not merely an academic pursuit but a practical necessity for real-world applications such as retrieval-augmented generation (RAG), agentic workflows, and intricate reasoning tasks. Historically, state space models (SSMs) have provided a basis for linear-scaling alternatives to attention. Early SSMs like S4 struggled with selective copying, but Mamba-1 introduced the ability to selectively attend to tokens, improving performance. The breakthrough Mamba-2 paper further optimized performance by reformulating SSMs as matrix transformations and demonstrating their equivalence to linear attention, a mechanism that also scales linearly with sequence length. vLLM V1's key advancement for hybrid models is a unified allocator that efficiently manages both the paged KV cache for attention layers and the fixed-size Mamba state. This unified approach replaces the problematic separate allocation in V0, which often led to memory errors or reduced concurrency. The unified allocator in V1 also paves the way for advanced features like prefix caching, KV cache transfer, and improved scheduling. Implementing the unified allocator for Mamba-based hybrid models presented challenges, primarily due to the disparate page sizes between attention blocks and Mamba states. This was overcome by aligning the attention block sizes with Mamba's larger page size and introducing minor padding. Further refinements involved adjusting the strides of Mamba state tensors to ensure compatibility with attention views, preventing data corruption when different layer types share the same memory tensors. To fully deprecate V0, extensive performance engineering was undertaken. This included refactoring modeling code and optimizing Triton kernels, particularly by implementing staged support for CUDA Graphs. This optimization was crucial for mitigating CPU launch overheads, especially in low-latency scenarios. The `FULL_AND_PIECEWISE` CUDA Graph scheme proved particularly effective, enabling V1 to consistently outperform V0, sometimes with throughput gains as high as 91%, especially for MoE models like `granite-4.0-h-tiny`. Benchmarks conducted on H100 GPUs comparing vLLM v0.10.2 (supporting both V0 and V1) demonstrated that V1 generally delivered better time-to-first-token (TTFT) and inter-token latency (ITL), along with significant throughput improvements. These results underscore that hybrid models are now robust and practical tools within vLLM V1, ready to handle the demands of next-generation, large-scale, long-context AI workloads, making them essential for enterprise-ready AI systems.

