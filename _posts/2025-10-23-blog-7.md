---
layout: post 
title: "Solving virtual machine puzzles: How AI is optimizing cloud computing"
blog_url: https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/?utm_source=tldrai 
---



## Key Points

- Google Research introduces LAVA, a new AI-powered scheduling algorithm for optimizing cloud computing resource efficiency by continuously repredicting VM lifetimes.
- LAVA addresses the 
bin packing problem
 of allocating virtual machines (VMs) onto physical servers for better resource utilization.
- The system includes three core algorithms: NILAS (Non-Invasive Lifetime Aware Scheduling), LAVA (Lifetime-Aware VM Allocation), and LARS (Lifetime-Aware Rescheduling).
- NILAS, deployed in Google's production data centers, has increased empty hosts by 2.3â€“9.2 percentage points and reduced CPU/memory stranding.
- LAVA strategically places shorter-lived VMs on hosts with long-lived VMs to fill resource gaps and minimize fragmentation.
- LARS minimizes VM disruptions during defragmentation and maintenance by prioritizing the migration of longer-lived VMs.
- The ML model is compiled directly into Google's Borg scheduler for low latency (9 microseconds) and high reliability, demonstrating a novel approach to deploying ML at scale.
- This work is a foundational step towards future data center management optimized by machine learning, showing significant efficiency gains without sacrificing reliability or latency.

## Key Topics Discussed

Alright podcast listeners, imagine a giant, constantly shifting Tetris game, but instead of blocks, you're packing virtual machines onto physical servers in massive cloud data centers. That's the complex challenge Google Research is tackling with LAVA, a brand-new AI-powered scheduling algorithm designed to supercharge cloud computing efficiency. Traditionally, guessing a VM's lifespan once at the start often leads to wasted resources, but LAVA changes the game with what they call 'continuous reprediction.' This means their machine learning model is always updating its estimate of a VM's remaining life, using probability distributions rather than a single guess, which is especially smart given how unpredictable VM lifespans can be. 

LAVA is actually a trio of clever algorithms. First up is NILAS, or Non-Invasive Lifetime Aware Scheduling. This one's already hard at work in Google's production data centers, and the results are pretty impressive. They've seen an increase in empty hosts by 2.3 to 9.2 percentage points, and a reduction in CPU and memory stranding, meaning more resources are actually being used! Then there's LAVA itself, Lifetime-Aware VM Allocation. This algorithm gets strategic, placing shorter-lived VMs on hosts that also have long-lived ones. The goal here is to fill those resource gaps efficiently, minimize fragmentation, and make sure hosts eventually get freed up. Simulations suggest LAVA could boost efficiency even further. And finally, we have LARS, or Lifetime-Aware Rescheduling. When it's time for maintenance or defragmentation, LARS smartly migrates the longest-lived VMs first, letting the shorter ones naturally finish their run. This approach could reduce VM live migrations by around 4.5%.

One of the really cool parts about this, and a major engineering feat, is how they deployed the ML model. Instead of relying on separate inference servers, which could create a tricky dependency, they compiled the model directly into Google's Borg scheduler. This not only makes it incredibly reliable but also blazing fast, with a median latency of just 9 microseconds! That speed is crucial for those constant repredictions and for performance-sensitive tasks. This whole project represents a significant leap forward in optimizing data center management with machine learning. It proves you can integrate advanced AI right into the core infrastructure, delivering big efficiency gains without compromising reliability or speed. This is truly a foundational step towards an even smarter, more efficient cloud computing future!

