---
layout: post 
title: "Why Cohereâ€™s ex-AI research lead is betting against the scaling race"
blog_url: https://techcrunch.com/2025/10/22/why-coheres-ex-ai-research-lead-is-betting-against-the-scaling-race/?utm_source=tldrai 
---



## Key Points

Sara Hooker, former VP of AI Research at Cohere, has launched Adaption Labs, challenging the industry's focus on scaling large language models (LLMs).
Hooker argues that the current approach of endlessly scaling LLMs is inefficient and hasn't led to AI that can effectively interact with the real world.
Adaption Labs aims to build AI systems that can continuously adapt and learn efficiently from real-world experiences.
A growing number of AI researchers, including Richard Sutton and Andrej Karpathy, share skepticism about the long-term potential of LLM scaling and current reinforcement learning methods.
The startup seeks to make learning from experience cheaper and more accessible, potentially democratizing AI control.
Hooker's previous work at Cohere Labs focused on compact AI systems that often outperform larger models.
If Adaption Labs' approach proves successful, it could significantly shift the direction of AI development away from a sole reliance on massive scaling.

## Key Topics Discussed

Alright, podcast listeners, let's dive into some fascinating news from the world of AI! We're talking about Sara Hooker, the former VP of AI Research at Cohere and a Google Brain alumna, who's making waves with her new startup, Adaption Labs. Hooker is essentially betting against the current AI industry's obsession with endlessly scaling large language models, or LLMs. She's challenging the idea that simply throwing more computing power at these models will lead to true superintelligence.

Hooker believes that the current scaling methods are becoming incredibly inefficient and haven't actually produced AI that can genuinely navigate and interact with our messy, real world. Adaption Labs is all about building AI systems that can continuously adapt and learn from their real-world experiences, doing so in a super efficient way. Think about it: if you stub your toe, you learn to be more careful next time, but current AI often just keeps stubbing its toe, so to speak.

This isn't just Hooker's solitary view either. There's a growing chorus of skepticism within the AI research community. Even big names like Richard Sutton, often called the 'father of reinforcement learning,' and early OpenAI employee Andrej Karpathy, have voiced concerns about the limitations of scaling LLMs and the long-term potential of current reinforcement learning methods. While there have been breakthroughs in AI reasoning models, they often come with a hefty computational price tag.

Adaption Labs, however, is trying to find a cheaper, more effective way for AI to learn from experience. This could be a game-changer, potentially challenging who controls and shapes AI by making advanced learning more accessible. Hooker's background includes leading Cohere Labs, where she focused on developing compact AI systems that often outperform their larger counterparts on specific tasks. If Adaption Labs is right, and adaptive learning proves more powerful and efficient than sheer scale, it could completely reorient the billions of dollars currently being invested in the scaling race. This is definitely a story to keep an eye on as the AI landscape continues to evolve!

