---
layout: post 
title: GPT-5 Prompting Guide
blog_url: https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide?utm_source=tldrai 
---

## Overview

OpenAI's GPT-5 prompting guide provides strategies to maximize the model's performance, particularly for agentic tasks, coding, and instruction adherence. It introduces new API parameters like `verbosity` and `reasoning_effort`, and emphasizes structured prompting, self-reflection, and clear instruction hierarchies to optimize outputs and agent autonomy. The guide also highlights GPT-5's ability to act as its own meta-prompter for continuous improvement.

## Key Points

GPT-5 offers significant improvements in agentic tasks, coding, and steerability.
New API parameters `reasoning_effort` and `verbosity` control exploration depth/efficiency and output length, respectively.
For agentic tasks, prompt explicitly for desired behavior (proactivity vs. explicit guidance) and use the `Responses API` for persistent reasoning.
Structured prompts with clear stop conditions, tool preambles, and self-reflection rubrics enhance output quality and user experience.
For coding, GPT-5 excels in large codebases; provide guiding principles, stack defaults, and UI/UX best practices in prompts.
Avoid contradictory or vague instructions, as GPT-5 meticulously follows prompts, which can lead to inefficiencies.
GPT-5 can function as a meta-prompter, helping users optimize their own prompts for desired behaviors.

## Key Topics Discussed

OpenAI's GPT-5, their latest flagship model, represents a significant advancement in AI capabilities, particularly in agentic task performance, coding, raw intelligence, and steerability. This prompting guide offers practical tips and best practices to help users maximize the quality of GPT-5's outputs, drawing from OpenAI's experience in training and applying the model to real-world scenarios. The guide covers crucial concepts such as enhancing agentic task performance, ensuring strict instruction adherence, leveraging new API features, and optimizing coding for both frontend and software engineering applications, including insights from AI code editor Cursor's work with GPT-5. A key theme is tailoring GPT-5's agentic behavior. By default, the model is thorough in gathering context, but users can control its proactivity. The `reasoning_effort` API parameter allows users to adjust exploration depth and efficiency; a lower setting improves latency for many workflows while still yielding consistent results. Conversely, increasing `reasoning_effort` encourages greater model autonomy and persistence in tool-calling. Prompting strategies are also vital, such as defining clear criteria for exploration or even setting fixed tool call budgets to limit tangential actions. When limiting context gathering, providing an "escape hatch" in the prompt, like allowing the model to proceed under uncertainty, can be beneficial. For increased autonomy, prompts should encourage persistence and thorough task completion, with the model continuing until the query is fully resolved without deferring to the user for every uncertainty. Clearly stating stop conditions for agentic tasks and defining safe versus unsafe actions is also emphasized. For interactive user experiences, GPT-5 is trained to provide clear upfront plans and consistent progress updates via "tool preamble" messages. Users can steer the frequency, style, and content of these preambles within their prompts, from detailed explanations of every tool call to brief initial plans. The guide strongly recommends using the new `Responses API` with GPT-5 to unlock improved agentic flows, lower costs, and more efficient token usage, noting statistically significant performance improvements in evaluations by persisting reasoning between tool calls. GPT-5 also exhibits leading coding capabilities, excelling in fixing bugs, handling large diffs, implementing multi-file refactors, and building entire new applications. For new app development, the guide suggests using self-constructed "excellence rubrics" within prompts to leverage GPT-5's planning and self-reflection. When implementing changes in existing apps, prompts should guide the model to adhere to existing style and design standards by summarizing key engineering principles, directory structure, and best practices. The guide provides example prompt snippets for organizing code editing rules, emphasizing clarity, reusability, consistency, and visual quality. A crucial point highlighted is GPT-5's precise instruction following. While this enables flexibility, poorly constructed prompts with contradictory or vague instructions can be detrimental, causing the model to expend reasoning tokens trying to reconcile conflicts. The guide provides adversarial examples of such prompts and demonstrates how resolving these instruction hierarchy conflicts can significantly improve GPT-5's reasoning and performance. Finally, GPT-5 introduces a new `verbosity` API parameter, which controls the length of the model's final answer independently of its thinking process. This parameter can be overridden by natural-language instructions within the prompt for specific contexts, allowing for granular control over output verbosity. A meta-point from early testers is GPT-5's ability to act as its own meta-prompter, helping users optimize their prompts by asking the model for suggestions on what to add or remove to achieve desired behaviors or prevent undesired ones. This iterative approach using GPT-5 itself underscores a powerful method for continuous prompt improvement.

