---
layout: post 
title: "INTELLECT-2 Release: The First 32B Parameter Model Trained Through Globally Distributed Reinforcement Learning"
blog_url: https://www.primeintellect.ai/blog/intellect-2-release?utm_source=tldrai 
---



## Key Points

INTELLECT-2 is the first 32B parameter model trained via globally distributed reinforcement learning.
It utilizes a fully asynchronous reinforcement learning approach across a decentralized network of compute contributors.
New open-source infrastructure components were developed: PRIME-RL for distributed asynchronous RL, TOPLOC for verifying untrusted rollouts, and SHARDCAST for efficient policy weight broadcasting.
The training recipe includes modifications to GRPO, such as Two-Sided GRPO Clipping and Advanced Data Filtering, crucial for stability and learning.
The project open-sources INTELLECT-2, its code, and data to foster open research in decentralized training.
Experiments demonstrated significant improvements in task rewards and increased performance on mathematics and coding benchmarks compared to QwQ-32B.
Future work aims to increase inference-to-training compute ratio, integrate tool calls and multi-turn RL, crowdsource RL tasks, and explore model merging.
INTELLECT-2 proves the efficacy of globally decentralized reinforcement learning.
The training infrastructure enables training across heterogeneous, unreliable networks.

## Key Topics Discussed

Prime Intellect has released INTELLECT-2, marking it as the first 32B parameter model to be trained through globally distributed reinforcement learning. This groundbreaking approach diverges from traditional centralized training by employing fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors. To achieve this, the team developed several bespoke open-source components, including PRIME-RL, a training framework designed specifically for distributed asynchronous reinforcement learning; TOPLOC, a locality-sensitive hashing scheme for verifiable inference that detects tampering; and SHARDCAST, a library for efficiently broadcasting updated model weights to decentralized inference workers. 

The training infrastructure also includes a Protocol Testnet for aggregating and coordinating global compute resources. Beyond infrastructure, the team implemented crucial modifications to the standard GRPO training recipe and data filtering techniques, such as Two-Sided GRPO Clipping, Advanced Data Filtering, and Aggressive Gradient Clipping, to ensure training stability and successful model learning on 285k verifiable tasks from various datasets. 

Experimental results from TARGET-SHORT and TARGET-LONG runs demonstrated significant improvements in task rewards, indicating enhanced performance on mathematics and coding problems. The model also showed increased performance over the previously trained QwQ-32B on benchmarks, although further generalized improvements may require better base models or higher-quality datasets. Looking ahead, Prime Intellect plans to further develop INTELLECT-2 by increasing the ratio of inference to training compute, incorporating tool calls and multi-turn reinforcement learning, crowdsourcing new RL tasks and environments, and exploring model merging techniques like DiLoCo. The release of INTELLECT-2 signifies a major step towards open frontier reasoning models trained in a decentralized fashion, showcasing that globally decentralized RL is indeed viable.

