---
layout: post 
title: "AI can improve on code it writes, but you have to know how to ask"
blog_url: https://www.theregister.com/2025/01/07/ai_can_write_improved_code_research/ 
---



## Key Points

- Large language models (LLMs) can generate better code when prompted iteratively, but the process benefits from developer expertise.
- Max Woolf’s experiment asked Anthropic’s Claude to solve a numeric‑digit‑sum problem; the initial solution ran in 657 ms on an Apple M3 Pro.
- A simple "make the code better" prompt yielded a 2.7× speedup; subsequent prompts added multithreading and achieved up to a 99.7× improvement, though later versions introduced bugs.
- Prompt engineering—providing detailed instructions and examples—produced faster, more sophisticated code more consistently, but also increased the likelihood of subtle errors.
- A separate study from Northeastern, Wellesley, and Oberlin found that the substance of prompts (the information they contain) matters more than phrasing, reinforcing that experienced developers get better results.
- The overall conclusion: LLMs can assist, but they won’t replace software engineers; human oversight remains essential to catch bugs and apply domain knowledge.

## Key Topics Discussed

Hey folks, today we’re diving into a fascinating Register piece that explores just how far AI can go when it writes its own code. Max Woolf, a senior data scientist at BuzzFeed, ran a series of prompts against Anthropic’s Claude to see if the model could iteratively improve a Python script that finds the difference between the smallest and largest numbers whose digits sum to 30. The first version was a straightforward, novice‑level solution that took about 657 ms on an Apple M3 Pro MacBook. When Woolf asked Claude to "make the code better," the model churned out an optimized version that ran 2.7 times faster. A second prompt added multithreading, pushing performance to a 5.1× gain, though it also introduced bugs that needed fixing. Further iterations produced speedups of 4.1× and a staggering 99.7× over the original, but each step also raised the risk of subtle errors.

Woolf didn’t stop there—he tried "prompt engineering," giving Claude more detailed instructions and examples via the system prompt. This approach yielded even faster, more sophisticated code, but again with a higher bug count. The takeaway? Simple iterative prompts do improve code, but targeted, information‑rich prompts accelerate gains more reliably, albeit with a trade‑off in reliability.

The article also references a new research paper from Northeastern, Wellesley, and Oberlin that examined whether the wording or the substance of prompts matters more for beginner programmers. The authors concluded that the information content—what you actually ask for—outweighs clever phrasing. In plain English, if you’re a seasoned developer who knows the problem domain, you’ll coax better results from an LLM than a newcomer who’s still learning the ropes.

Bottom line: AI can be a powerful co‑pilot for coding, but it still needs a human pilot to steer, catch bugs, and apply nuanced domain knowledge. So while LLMs won’t replace software engineers anytime soon, they can give experienced devs a serious productivity boost—provided they know how to ask the right questions.

