---
layout: post 
title: "Humans Don't Have World Models"
blog_url: https://danielmiessler.com/blog/humans-dont-have-world-model?utm_source=rss&amp;utm_medium=feed&amp;utm_campaign=website 
---



## Key Points

The article questions the assumption that humans possess world models, similar to how AI researchers believe advanced AI needs them.
The author argues that human articulation of how the world works, like LLMs, appears to be a real-time generation of images and text, not a retrieval from a pre-existing, coherent 'world model.'
The quality of a human's understanding (their 'world model') is dependent on their training and experience, drawing a parallel to how LLMs are trained.
The article highlights the 'black box' nature of human thought, suggesting that our responses and ideas often 'pop into our head' without conscious control, much like an AI streaming tokens.
The author posits that if human internal processes are as opaque and generative as those of LLMs, then our standard for free will, agency, and world models in humans might need re-evaluation.

## Key Topics Discussed

The article challenges the prevailing notion that humans possess explicit 'world models,' unlike large language models (LLMs). The author argues that the way humans articulate their understanding of the world, often through a stream of images and words generated in real-time without prior formulation, bears striking resemblance to how LLMs produce responses. The core argument is that just as LLMs generate text based on their training data, humans retrieve and articulate their understanding based on their experiences and learning. The author points out that the quality of a human's 'world model' depends on their training, similar to how an LLM's output quality depends on its training data. The piece further delves into the 'black box' nature of human consciousness, suggesting that our thoughts and responses often emerge without conscious control, akin to an AI streaming word tokens. This perspective leads to a provocative conclusion that our understanding of concepts like free will, agency, and even world models in humans might be based on a misinterpretation of our internal cognitive processes.

