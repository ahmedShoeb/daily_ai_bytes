---
layout: post 
title: "It's Hard to Feel the AGI"
blog_url: https://tensorlabbet.com/2025/11/30/hard-to-feel-agi/?utm_source=tldrai 
---



## Key Points

Ilya Sutskever, former chief scientist of OpenAI, believes current transformer-based LLMs will stall, requiring new research for AGI, and he has revised his AGI timeline back by 5-20 years.
Sutskever also expresses doubts about the future profitability of current LLM business models due to a lack of differentiation.
Andrej Karpathy critiques the hype around LLM-based AI Agents, stating they need a decade of work and improvements as they are currently 'cognitively lacking.'
Rich Sutton argues that LLMs are a dead end in AI research, lacking an internal 'world model' and continual learning abilities, only mimicking language through imitation.
Yann LeCun asserts that language is not intelligence and LLMs lack an adequate mental model of the physical world, limiting their cognitive abilities below even young children or animals.
A growing consensus among AI experts, including those previously optimistic, indicates significant limitations in current AI approaches and a revised, longer timeline for AGI.
Despite tangible achievements of LLMs in specific tasks, there's a risk of an 'AI Winter' if investors become disillusioned by inflated expectations conflating machine learning with human-like robots.

## Key Topics Discussed

Hey everyone, welcome back to the podcast! Today, we're diving into a fascinating discussion about the current state of AI and the evolving perceptions around Artificial General Intelligence, or AGI. It seems there's a chilling effect spreading as the cracks between marketing hype and technological reality become more apparent. Many leading minds in the field are actually revising their projections for the near future of AI. 

Ilya Sutskever, the former chief scientist of OpenAI, recently shared his view that our current approach with transformer-based Large Language Models, or LLMs, is likely to hit a plateau soon. He believes we'll need fundamentally new research insights to break through this ceiling. He's even pushed back his estimate for human-like learning abilities by a good 5 to 20 years! He also raised concerns about the long-term profitability of current LLM business models, given the lack of clear differentiation between competitors. 

Andrej Karpathy, another influential voice, echoed some of these sentiments, offering a noteworthy critique of the industry hype surrounding LLM-based AI Agents. While he finds them impressive, he believes this technology still needs a decade of intense work and improvements. He argues that they're currently 'cognitively lacking' and not yet performing like the automated employees or coworkers we've been promised. Instead, he anticipates a more gradual economic growth from these systems, much like the long-term compounding patterns we've seen since the Industrial Revolution, rather than a sudden leap. He even compared this slower development to earlier ambitions for fully automating radiology and self-driving cars, both of which, despite impressive early demos, haven't quite reached their fully autonomous goals yet. 

Then there's Rich Sutton, who has taken a rather contentious stance, suggesting that LLMs are actually a dead end for AI research. He argues that while they're surprisingly effective, LLMs don't possess an internal 'world model' that would allow them to explore actions and predict outcomes. Instead, he believes they simply mimic human language through imitation. He points out their lack of any actual goals beyond mechanistically processing tokens and highlights their critical deficiency in continual learningâ€”an ability he considers essential for navigating our complex world. 

Yann LeCun, a long-standing critic of the idea that LLMs could scale to human-level intelligence, largely agrees with these points. He powerfully states that language is not intelligence. He views language as a low-bandwidth modality, whereas the physical world is experienced with high bandwidth through continuous, high-dimensional representations. He argues that LLMs fundamentally lack an adequate mental model of the physical world needed for planning sequences of actions towards a goal, limiting their cognitive abilities to below that of even young children or animals without language. LeCun expects truly intelligent systems to acquire common sense and an understanding of the physical world from multimodal inputs, operate with persistent memory, and perform reasoning and planning. He sees parallels between the current exuberance around LLMs and the hype around expert systems in the 1980s, which also led to high expectations followed by disillusionment. 

So, what does all this mean for us? Well, these findings might not be entirely unexpected for those of us who have been following the field closely. What's more surprising is the consensus that's taking hold even among those who were previously much more optimistic, sometimes with financial incentives at play. To be clear, LLMs and other generative models have achieved many tangible and valuable things, from generating text, images, and video to brainstorming, planning, and summarization. They can certainly provide genuine value for years to come, especially with varying degrees of human supervision for tasks like software engineering. 

However, pinpointing the limits of their autonomy will remain a challenging, moving target. The big concern is that this nuance won't be understood, and the industry could face a new 'AI Winter' among disillusioned investors who might have massively bought into what felt like science fiction, confusing machine learning with truly human-like robots. It's a reminder that while the progress in AI is incredible, a healthy dose of realism and critical evaluation is always necessary. We need continued innovation from both industry and academia to gradually develop the capabilities required for what we truly consider intelligence.

