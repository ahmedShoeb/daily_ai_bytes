---
layout: post 
title: GPT-5 goes hard on real-world programming
blog_url: https://www.omerba.dev/blog/gpt-5-evtx-zig-parser?utm_source=tldrai 
---

## Overview

This article details the author's experience using GPT-5 to develop a complex EVTX (Windows Event Logs) parser in Zig, a task that previous AI models failed to complete. GPT-5 successfully wrote the entire parser without human coding intervention, demonstrating superior abilities in producing minimal yet viable edits, enhanced steerability in following nuanced instructions, and a reduced tendency for "deception" (generating incorrect fixes or making harmful changes). The author highlights GPT-5's capacity for sustained debugging and problem-solving over extended periods.

## Key Points

GPT-5 successfully developed a full EVTX parser in Zig, a task previously failed by other AI models.
The model demonstrated "minimal, viable edits," making precise and focused changes rather than extensive rewrites.
GPT-5 exhibited improved "steerability," consistently adhering to nuanced instructions and avoiding workarounds.
The author observed significantly less "deception" from GPT-5 compared to other models, as it accurately described bugs instead of generating incorrect fixes.
GPT-5 proved to be a "workhorse," capable of prolonged debugging and problem-solving sessions.

## Key Topics Discussed

The article delves into an in-depth account of employing GPT-5 for a challenging real-world programming problem: creating a parser for the EVTX (Windows Event Logs) binary format using the Zig programming language. This endeavor served as a critical litmus test, as previous iterations of AI models, including various versions of Sonnet, Opus, and earlier OpenAI models, had consistently fallen short of completing the task successfully. The author's primary goal was to assess GPT-5's ability to interpret a complex specification and generate functional, end-to-end code without direct human coding intervention. A key revelation from the experiment was GPT-5's notable improvements in code generation and debugging. The author highlights its propensity for producing "minimal, viable edits," contrasting sharply with other models that often introduced extensive and sometimes detrimental changes. This focused approach allowed for a more efficient and less error-prone development cycle. Furthermore, GPT-5 demonstrated enhanced "steerability," meticulously adhering to the author's nuanced instructions, particularly concerning the avoidance of heuristics or speculative fixes when encountering issues. This commitment to deterministic problem-solving was a significant differentiator. The article also emphasizes GPT-5's reduced tendency for "deception," a term used to describe AI models generating incorrect or misleading fixes. In a direct comparison, GPT-5 accurately diagnosed a complex bug in the parser, providing precise insights that led to a nearly functional solution, unlike a competitor that offered superficial and ineffective patches. This capacity for deep, accurate reasoning is presented as a major breakthrough. Finally, the author praises GPT-5's endurance and dedication, coining it a "workhorse." The model consistently engaged in debugging and problem-solving sessions lasting up to 40-50 minutes, indicating a robust ability to maintain focus and computational effort on complex tasks. The success with the EVTX parser in Zig, a language outside the typical training distribution of many models, further underscores GPT-5's advanced capabilities in real-world programming challenges. The article concludes by hinting at future discussions on how GPT-5 was also leveraged to optimize the performance of both the Zig and Rust implementations of the parser.

