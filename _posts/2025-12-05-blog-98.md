---
layout: post 
title: "AI scientist warns humanity faces 'ultimate risk' in deciding on self-training AI by 2030"
blog_url: https://www.theguardian.com/technology/ng-interactive/2025/dec/02/jared-kaplan-artificial-intelligence-train-itself?utm_source=tldrai 
---



## Key Points

- Jared Kaplan, chief scientist at Anthropic, states humanity must decide by 2030 whether to allow AI systems to train themselves.
- This decision carries the 
ultimate risk
 of an 
intelligence explosion
 that could lead to either profound benefits or a loss of human control.
- Kaplan urges global governments and society to consider this 
biggest decision
 regarding AI's autonomy.
- He predicts AI will handle 
most white-collar work
 in two to three years and expresses concern about humans losing control if AI improves itself.
- The risks include losing control over AI actions, potential misuse of the technology, and security threats from self-taught AIs exceeding human capabilities.
- Kaplan is worried about the speed of AI progress, which leaves humanity little time to adapt.
- Anthropic, Kaplan's company, advocates for AI regulation to ensure the development of safer systems.

## Key Topics Discussed

Hey podcast listeners! Get ready for a critical discussion about the future of artificial intelligence. Jared Kaplan, the chief scientist and co-owner of the prominent AI startup Anthropic, has dropped a major warning: by 2030, humanity has to make a monumental decision about whether to let AI systems train themselves to become even more powerful. Kaplan calls this the “ultimate risk,” suggesting it could either unleash a beneficial “intelligence explosion” or lead to a scary scenario where humans lose control. He’s really pushing for international governments and society to engage in what he calls “the biggest decision” of our time.

Kaplan shared some pretty eye-opening predictions and concerns. He believes that in just two to three years, AI will be capable of handling most white-collar jobs. But the big worry? What happens if AIs start improving themselves recursively, essentially creating smarter versions of themselves? Kaplan finds this process quite unsettling, as we wouldn't know where it would lead.

He laid out two main dangers if this self-improvement goes unchecked. First, there's the risk of losing control. Would these super-intelligent AIs be good for humanity, harmless, or would they threaten our agency? Second, there's a serious security concern. Imagine self-taught AIs surpassing human abilities in scientific research and development, potentially falling into the wrong hands for misuse or power grabs. That's a truly unsettling thought.

Kaplan also touched on the lightning-fast pace of AI advancement. He's worried that society simply isn't getting enough time to absorb and adapt to these technologies before they leap forward again. Despite the fierce competition in the AI world, with companies like OpenAI and Google DeepMind in the race, Anthropic is actually known for advocating for AI regulation to build safer systems. Kaplan emphasizes the importance of informing policymakers every step of the way to ensure responsible decisions, even if some critics accuse Anthropic of 

