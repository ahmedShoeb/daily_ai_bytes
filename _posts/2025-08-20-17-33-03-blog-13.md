---
layout: post 
title: MolmoAct: An Action Reasoning Model that reasons in 3D space
blog_url: https://allenai.org/blog/molmoact?utm_source=tldrai 
---

## Overview

AI2 introduces MolmoAct, the first open-source Action Reasoning Model (ARM) capable of "thinking" in three dimensions. Built on the Molmo vision-language model family, MolmoAct addresses the limitations of language models in spatial reasoning, enabling machines to precisely follow instructions and plan actions in 3D environments. It achieves this by understanding the physical world through depth-aware perception tokens, planning in image space via waypoints, and decoding actions into low-level motor commands for robotics hardware.

## Key Points

MolmoAct is the first open-source Action Reasoning Model (ARM) designed for 3D spatial reasoning.
It overcomes limitations of traditional vision-language models (VLAs) that primarily reason in written language.
MolmoAct operates in three stages: understanding the physical world via depth-aware perception tokens, planning in image space with waypoints, and action decoding for hardware control.
It demonstrates strong generalizability across different robotic embodiments and tasks with minimal fine-tuning.
The model and its curated post-training dataset, containing ~10,000 robot episodes, are openly available for research.
MolmoAct-7B was trained efficiently and achieves state-of-the-art performance on key robotics benchmarks like SimplerEnv and LIBERO.
It allows for intuitive control through natural language or visual traces, offering interpretability and steerability.

## Key Topics Discussed

AI2 has unveiled MolmoAct, a groundbreaking Action Reasoning Model (ARM) that marks a significant leap in how AI models interact with and reason about the three-dimensional world. Unlike conventional large language models (LLMs) and even many vision-language-action (VLA) models that primarily reason through written language, MolmoAct is designed to "think" in three dimensions. This innovation addresses a long-standing challenge where text alone often falls short in capturing the intricate complexities required for physical movement and spatial understanding, similar to how humans struggle to learn complex physical maneuvers solely from reading. MolmoAct, built upon AI2's open-source Molmo family of vision-language models, bridges the crucial gap between language and action. It enables machines to follow instructions with remarkable precision and reason effectively within 3D space. The model achieves this by grounding scene semantics through unique depth-aware perception tokens, which encode geometric structure and positional embeddings. This allows MolmoAct to accurately estimate distances between objects and integrate this spatial understanding into its reasoning process. The model's reasoning unfolds in three autoregressive stages. First, it comprehends the physical world by outputting spatially grounded perception tokens. Second, conditioned on these tokens, MolmoAct plans in image space, predicting a sequence of image-space waypoints that visually outline the task's progression, maintaining independence from a machine's specific embodiment. Finally, in the action decoding stage, MolmoAct translates these waypoints into low-level action commands for robotic hardware, such as end-effectors and grippers, which are then denormalized for precise motor control. This architecture allows MolmoAct to adapt to various robotic embodiments, from humanoids to gripper arms, and tasks more effectively than strong baselines with only minimal fine-tuning. In line with AI2's mission for open science, MolmoAct, including its first iteration MolmoAct-7B, is completely open source. This includes the model itself, its highly curated post-training dataset (MolmoAct post-training dataset), which comprises approximately 10,000 distinct "robot episodes" derived from videos of robots performing diverse household actions, and essential artifacts like evaluation scripts and benchmark tools. This commitment to openness allows for inspectability, verifiability, and reproducibility, fostering impactful research within the community. The post-training data, linking high-level reasoning to concrete actions and exposing each reasoning stage through action chain-of-thought sequences, is expected to be particularly valuable. MolmoAct-7B demonstrates remarkable efficiency and performance. It was trained far more efficiently than many VLA models used in robotics, yet it surpasses several leading models on key robotics benchmarks. For example, it achieved a state-of-the-art out-of-distribution task success rate of 72.1% on SimplerEnv and delivered state-of-the-art results on the LIBERO simulation for knowledge transfer, showcasing its strength as a powerful generalist foundation model that quickly adapts to new tasks, embodiments, and domains. Extensive real-world experiments further confirm MolmoAct's superior generalization ability, outperforming other models even when subjected to perturbations like paraphrased language commands or novel objects. Furthermore, MolmoAct offers enhanced interpretability and steerability. Users can intuitively control the model using natural language or by drawing a visual trace on a screen. Before executing commands, MolmoAct visually grounds its internal reasoning in pixel space and overlays its planned motion trajectory directly onto input images, providing a clear preview of intended movements. This visual trace offers a means to correct mistakes or prevent unwanted behaviors. Users can also guide MolmoAct in real-time by sketching target poses or paths on a device, leading to a safer and more explainable robotic experience. AI2 views MolmoAct as a strong foundation for future work, enabling coherent, grounded, and transparent behavior in robotics and other hardware, and plans to continue broadening its evaluation and conducting more real-world experiments.

