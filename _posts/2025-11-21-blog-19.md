---
layout: post 
title: "Breaking the Million-Token Barrier: The Technical Achievement of Azure ND GB300 v6"
blog_url: https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/breaking-the-million-token-barrier-the-technical-achievement-of-azure-nd-gb300-v/4466080?utm_source=tldrai 
---



## Key Points

Azure ND GB300 v6 Virtual Machines achieve 1,100,000 tokens/s on Llama2 70B Inference.
This performance is a 27% increase over the previous Azure ND GB200 v6 record of 865,000 tokens/s.
The new VMs are built on the NVIDIA Blackwell architecture, optimized for inference workloads with 50% more GPU memory and 16% higher TDP.
An NVL72 rack of Azure ND GB300 v6 achieved the aggregated 1,100,000 tokens/s for an unverified MLPerf Inference v5.1 submission.
Azure ND GB300 v6 VMs deliver 5x higher throughput per GPU than the previous-generation ND H100 v5 virtual machines.
The milestone was observed by Signal65, validating the performance for large-scale AI.
Performance gains are attributed to hardware components like GEMM efficiency, HBM throughput, NVLink connectivity, and NCCL communication.
The Llama2 70B model was run using FP4 precision, implemented via NVIDIA TensorRT-LLM library.

## Key Topics Discussed

Hey everyone! Get ready to be amazed, because Azure has just shattered some serious records in AI inference with their new ND GB300 v6 Virtual Machines! We're talking about an unprecedented performance of 1,100,000 tokens per second on Llama2 70B Inference. That's a massive 27% jump over their previous record-holder, the Azure ND GB200 v6, which was already super fast at 865,000 tokens per second! These aren't just any VMs; they're built on the cutting-edge NVIDIA Blackwell architecture, fine-tuned specifically for inference workloads. They come packed with 50% more GPU memory and a 16% higher Thermal Design Power, making them powerhouses for your AI needs. They ran the Llama2 70B model from MLPerf Inference v5.1, simulating real-world customer workloads, and one NVL72 rack of Azure ND GB300 v6 machines hit that incredible 1.1 million tokens per second mark! This translates to a blistering 15,200 tokens per second per NVIDIA Blackwell Ultra GPU, showing a significant speedup. Plus, these new ND GB300 v6 VMs offer five times higher throughput per GPU than the earlier ND H100 v5 generation. Third-party observer Signal65 confirmed this groundbreaking achievement, highlighting its importance for large-scale, transformative AI. The secret sauce behind these gains lies in optimized hardware components like GEMM efficiency, high-bandwidth memory throughput, NVLink connectivity, and NCCL communication. And to top it all off, the Llama2 70B model was executed with FP4 precision, a smart quantization technique that boosts inference speed without sacrificing accuracy, all thanks to the NVIDIA TensorRT-LLM library. Azure is seriously raising the bar for enterprise-scale AI inference, and it's exciting to see what this means for the future of AI!

