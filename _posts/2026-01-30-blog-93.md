---
layout: post 
title: "DEFOM-Stereo: Depth Foundation Model Based Stereo Matching"
blog_url: https://arxiv.org/abs/2501.09466v1 
---



## Key Points

This paper introduces DEFOM-Stereo, a new framework for stereo matching that incorporates a robust monocular relative depth model.
DEFOM-Stereo combines features from traditional CNNs and a depth foundation model (DEFOM) to improve feature extraction.
It uses the depth predicted by DEFOM to initialize disparity estimations and includes a scale update module for refinement.
The model demonstrates strong performance on datasets like Scene Flow, KITTI, Middlebury, and ETH3D, achieving state-of-the-art results.
DEFOM-Stereo exhibits excellent zero-shot generalization capabilities.

## Key Topics Discussed

Alright, let's break down this research paper on DEFOM-Stereo. It’s all about improving stereo matching, which is a core technique for getting accurate depth information in computer vision and robotics. Now, stereo matching isn't always easy. Things like objects blocking each other (occlusion) or surfaces lacking texture can throw it off. The researchers noticed that recent advances in estimating depth from a *single* image – using what they call ‘vision foundation models’ – have been really promising. So, they thought, what if we could bring those strengths into stereo matching? That’s where DEFOM-Stereo comes in. Essentially, they've built a framework that leverages a strong monocular depth model, DEFOM, alongside traditional stereo matching techniques. They’ve designed a clever feature extraction process, blending information from standard CNNs with the insights from DEFOM. Then, in the processing stage, they use DEFOM’s depth prediction as a starting point for refining the disparity, and also have a module to make sure the scale of the depth map is correct. The results are really impressive! They’ve shown DEFOM-Stereo performs as well as, and often better than, the current state-of-the-art methods on some tough benchmark datasets – including KITTI, Middlebury, and ETH3D. Importantly, it’s also shown to generalize well to new situations, meaning it can perform accurately even on data it hasn’t specifically been trained on. Overall, this looks like a significant step forward in robust and accurate depth estimation.

