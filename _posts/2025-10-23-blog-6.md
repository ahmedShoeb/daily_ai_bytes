---
layout: post 
title: "Can You Infinitely Learn with Online RL?"
blog_url: https://blog.sdan.io/geospot-infinity/?utm_source=tldrai 
---



## Key Points

- Geospot Infinity, a photo-to-GPS model, used online Reinforcement Learning (RL) to learn from user interactions.
- Over 65% of users clicked the first guess regardless of accuracy, causing the online RL policy to degrade output estimates by 414km (17% worse than baseline).
- The initial REINFORCE-based online RL policy optimized for user preference tuning rather than geodesic accuracy.
- The article references Meta's research on scaling RL for LLMs, indicating that RL ceilings are influenced by model size, context, batch size, and precision.
- Geospot Infinity was refactored to use Direct Preference Optimization (DPO) to improve learning from user clicks.
- The core challenge identified is 'learning to reward the reward' and how user behavior impacts the learning process.

## Key Topics Discussed

Alright podcast listeners, today we're diving into a fascinating journey with Geospot Infinity, a model designed to pinpoint a photo's location using online Reinforcement Learning, or RL. Now, the idea was brilliant: learn from every user click. But here's the kicker, folks: over 65% of users just clicked the first guess, no matter how accurate it was! This behavior completely sabotaged the learning process, leading to the RL policy actually making predictions 414 kilometers, or 17%, *worse* than the baseline. Can you believe it? The model, which initially used a tiny 3-layer MLP to re-rank candidate coordinates and a REINFORCE objective, ended up optimizing for user preference, not actual geodesic accuracy. The author then broadens our perspective by bringing in Meta's insights on scaling RL for large language models, highlighting that factors like model size, context length, batch size, and even precision are crucial in elevating the RL 'ceiling.' It's all about capacity, context, and getting that reward framing just right. To tackle the preference tuning problem, Geospot Infinity was eventually refactored to use Direct Preference Optimization, or DPO. This new approach aims to more effectively learn by comparing the clicked candidate against the lower-ranked ones. Ultimately, the article really drives home the idea that the true challenge in these online learning systems isn't just the algorithms themselves, but 'learning how to reward the reward' and understanding how user behavior shapes the entire learning landscape.

