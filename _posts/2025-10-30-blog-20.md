---
layout: post 
title: "On-Policy Distillation"
blog_url: https://thinkingmachines.ai/blog/on-policy-distillation/?utm_source=tldrai 
---



## Key Points

Large Language Models (LLMs) training involves three stages: pre-training, mid-training for domain knowledge, and post-training for targeted behavior.
Smaller, specialized models often outperform larger general models, offering benefits in deployment, updates, and inference costs.
Post-training approaches include on-policy (e.g., Reinforcement Learning, RL) and off-policy (e.g., Supervised Fine-Tuning, SFT, or distillation) methods.
RL provides on-policy relevance but is inefficient due to sparse feedback, while off-policy distillation offers dense feedback but can cause compounding errors and style imitation over factual accuracy.
On-policy distillation combines on-policy sampling from the student with dense, token-level grading from a high-performing teacher model.
This method offers significant compute savings (e.g., 9-30x reduction) and is more data-efficient than traditional RL or off-policy SFT.
On-policy distillation is effective for improving mathematical reasoning and for personalizing models, such as creating internal company assistants.
It's a promising approach for continual learning, allowing models to acquire new knowledge without degrading previously learned behaviors, addressing catastrophic forgetting.
The technique utilizes a reverse KL divergence loss function to push the student to approximate the teacher's behavior.

## Key Topics Discussed

Hey everyone, today we're diving into a fascinating topic from Thinking Machines: 'On-Policy Distillation.' This article really breaks down the advanced stages of training Large Language Models, or LLMs, particularly focusing on what happens after the initial pre-training. You see, LLMs go through three main stages: first, general pre-training, then mid-training for specific domain knowledge, and finally, post-training to get them to perform targeted tasks. The cool thing is, often smaller, more specialized models, when trained well, can actually outperform those massive general-purpose models, bringing benefits like easier local deployment, simpler updates, and reduced inference costs. 

The article then contrasts two primary post-training methods: on-policy and off-policy training. On-policy, like Reinforcement Learning, means the model learns from its own actions. While this is super relevant to how the model actually behaves, the feedback is really sparseâ€”you only get a 'win' or 'lose' signal at the end, which isn't very efficient. Then there's off-policy training, which often involves something called distillation, where a student model learns by imitating a high-performing 'teacher' model. Here, you get dense feedback, but it comes with its own set of problems. The student might make an early mistake that the teacher never made, leading to compounding errors, or it might just mimic the teacher's style without necessarily grasping the factual accuracy.

So, what's the solution? Enter on-policy distillation! This innovative approach takes the best of both worlds. It involves the student model generating its own trajectories, and then a powerful teacher model steps in to grade *each individual token* in that trajectory. This means you get the dense, detailed feedback of distillation, combined with the real-world relevance of on-policy training. The results are pretty impressive: the article shows significant compute savings, sometimes a 9 to 30 times reduction in cost for tasks like math reasoning compared to traditional methods.

Beyond just efficiency, on-policy distillation is also a game-changer for personalizing models, like creating a smart internal assistant that understands your company's documents and follows instructions perfectly. It can even help prevent 'catastrophic forgetting,' allowing models to continuously learn new information without losing their existing skills. The technical magic behind this involves a reverse KL divergence loss function, which basically makes the student model mimic the teacher's desired behavior at every step. In a nutshell, on-policy distillation is presented as a powerful, cost-effective tool for pushing the boundaries of LLM capabilities in that crucial post-training phase.

