---
layout: post 
title: "Why do LLMs freak out over the seahorse emoji?"
blog_url: https://vgel.me/posts/seahorse/?utm_source=tldrai 
---



## Key Points

LLMs, including GPT-5 and Claude Sonnet, are consistently confident about the existence of a seahorse emoji, despite it not being in Unicode.
This mistaken belief likely originates from human training data or a logical generalization given the presence of other aquatic animal emojis.
When trying to output a seahorse emoji, LLMs can exhibit unusual behaviors, such as outputting incorrect emojis or entering 

## Key Topics Discussed

The article explores why large language models (LLMs) consistently believe in the existence of a seahorse emoji, even though it doesn't exist. Popular models like GPT-5 and Claude Sonnet confidently respond 'Yes' when asked if there is a seahorse emoji. This widespread conviction is not limited to LLMs; many humans also recall a seahorse emoji, leading to online discussions and even defunct memecoins about its supposed disappearance. The author speculates that this belief could be influenced by human training data, where many people incorrectly remember a seahorse emoji, or it could be a convergent belief, as it's a reasonable assumption given the abundance of other aquatic animal emojis in Unicode. A seahorse emoji was even formally proposed but rejected in 2018. The article uses a 'logit lens' to delve into the internal workings of LLMs when processing the seahorse emoji query. This interpretability tool reveals that in the middle layers of the model, a 'seahorse + emoji' concept is actively being constructed. The `lm_head` of an LLM aims to match these internal residual representations to existing token vectors to produce an output. For real emojis, like the fish emoji, the model successfully constructs a 'fish + emoji' vector, leading to the correct output. However, because no actual seahorse emoji token exists, the `lm_head` attempts to find the most similar existing emoji, often resulting in an incorrect output like a tropical fish or horse emoji. The article further explains that once an incorrect token is sampled and appended to the context, some models, like Claude 4.5 Sonnet, can recognize the error and correct themselves, while others, like gpt-5-chat, might continue to 'spiral' with incorrect outputs. The author speculates that reinforcement learning could be beneficial in addressing this issue by providing models with feedback about the limitations of their `lm_head` and helping them update their mistaken beliefs.

