---
layout: post 
title: "Thoughts on AI progress (Dec 2025)"
blog_url: https://www.dwarkesh.com/p/thoughts-on-ai-progress-dec-2025?utm_source=tldrai 
---



## Key Points

The author is moderately bearish on short-term AI progress but explosively bullish on long-term potential.
Current AI training methods, involving 'pre-baking' skills, contradict the idea of imminent AGI as a human-like learner.
Human labor is valuable due to its ability to learn on the job and generalize, a capability current AI models lack.
The slow economic diffusion of AI is attributed to a lack of core capabilities rather than just slow technological adoption.
The redefinition of AGI's goalposts is justified as AI models become impressive but still fall short of broad economic automation.
Continual learning, where AI agents learn from experience and share knowledge, is seen as the main driver for future AGI advancements.
Competition among AI labs is expected to remain intense, preventing any single entity from gaining a decisive, long-lasting advantage.

## Key Topics Discussed

Alright listeners, let's dive into some fascinating thoughts on AI progress from December 2025, as shared by Dwarkesh. Our author starts off with a nuanced take: moderately bearish on the short-term, but explosively bullish on the long-term prospects of artificial intelligence. One of the big points of confusion for him is the current approach to AI training, often called 'RLVR' or 'mid-training,' where labs are essentially 'pre-baking' a ton of skills into models. He argues that if we were truly close to a human-like learner, this entire approach would be fundamentally flawed. Humans don't need a special training phase for every single piece of software they might use; they learn on the job. The fact that AI still requires this extensive, skill-specific pre-baking suggests that true AGI, with its ability to learn self-directed, isn't as imminent as some might think. He even brings in insights from Beren, highlighting the billions spent on PhDs and experts to create specific training data, likening it to a modern expert systems era rather than true generalized learning. This is particularly vivid in robotics, where despite advanced hardware, the core learning algorithms are still missing. He also pushes back against the idea that a 'superhuman AI researcher' built through these clunky RL methods will suddenly solve robust and efficient learning from experience. It's like saying, 'We’re losing money on every sale, but we’ll make it up in volume.' He believes this automated researcher still needs basic learning capabilities that children possess. Our author emphasizes that human labor is valuable precisely because it doesn't require these elaborate, custom training loops for every micro-task. An AI that could truly learn from semantic feedback or self-directed experience, and then generalize like a human, is what's needed. Until then, automating entire jobs by simply baking in predefined skills is not feasible. In fact, he feels many are underestimating the impact of actual AGI, which he envisions as billions of human-like intelligences on a server, capable of copying and merging all their learnings – something he expects in the next decade or two. He then tackles the idea of 'economic diffusion lag,' calling it 'cope' for missing capabilities. He argues that if AI models were truly like humans on a server, they would integrate into the economy incredibly quickly, far faster than hiring a human. The current lack of trillions in revenue for AI labs, compared to the tens of trillions in human knowledge worker wages, clearly indicates that models are not yet as capable. The author also touches on the often-criticized 'goalpost shifting' by AI bears. He admits that some shifting is fair because AI has made immense progress. However, he argues it's totally reasonable to look at models like Gemini 3 and realize that while impressive, they haven't automated half of knowledge work as one might have expected in 2020. This indicates that our previous definition of AGI was too narrow, and he expects this redefinition to continue. He also notes that models are getting more impressive at the rate short-timeline advocates predict, but more *useful* at the rate long-timeline advocates predict. A crucial point is his skepticism about how 'RL scaling' is using the 'prestige' of pretraining scaling. Pretraining showed clean, general improvements, but RLVR lacks such predictable trends, with some research suggesting a massive scale-up is still needed. He makes an interesting comparison to human distribution: because knowledge work sees huge variance in human value-add, with top performers contributing disproportionately, comparing AI to the median human can overestimate its initial value. But once AI matches top human performance, the impact could be explosive. Finally, he talks about a 'broadly deployed intelligence explosion' driven not just by software or hardware, but primarily by *continual learning*. This isn't a one-time 'solve,' but a gradual progression, similar to how in-context learning evolved. He doesn't expect a runaway gain for the first model to crack continual learning, as breakthroughs will likely be replicated and improved upon by competitors. He also anticipates diminishing returns from learning-from-deployment and that competition among labs will remain fierce, preventing any single entity from maintaining a lasting advantage. This competition, he suggests, neutralizes any supposed 'flywheels' and ensures that the top labs rotate around the podium regularly. He even hints at a fascinating idea from a podcast with Ilya: the predictability of human misunderstandings helps learning, whereas AI's unpredictable mistakes make creating robust learning environments incredibly difficult, posing a major challenge for continual learning. And there you have it, folks – a deep dive into the evolving landscape of AI progress, highlighting the complexities and the exciting, yet challenging, road ahead!

