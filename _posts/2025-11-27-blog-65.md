---
layout: post 
title: "Universal LLM Memory Does Not Exist: Benchmarking Mem0 and Zep Reveals High Costs and Low Accuracy"
blog_url: https://fastpaca.com/blog/memory-isnt-one-thing?utm_source=tldrai 
---



## Key Points

A benchmark comparing Mem0 and Zep against naive long-context revealed LLM memory systems are 14-77x more expensive and 31-33% less accurate.
The high costs and latency are attributed to an 'LLM-on-Write' architecture, where multiple LLMs process every message for summarization and fact extraction.
Zep's knowledge graph system demonstrated an alarming token consumption, reaching 1.17 million tokens per test case due to recursive LLM calls.
Both Mem0 and Zep share a critical flaw: relying on non-deterministic LLMs for fact extraction, which can introduce hallucinations and corrupt data at the write stage.
The article argues that marketing often misleads by focusing on 'cost per retrieval' rather than the true 'cost per conversation.'
The author concludes that 'Universal LLM Memory' is a myth, differentiating between 'Semantic Memory' (fuzzy, for user) and 'Working Memory' (lossless, for agent), advocating for separate architectural approaches.

## Key Topics Discussed

Hey everyone, welcome back to the podcast! Today, we're diving into a really insightful piece that challenges some big assumptions about LLM memory systems. The article, 'Universal LLM Memory Does Not Exist' from fastpaca.com, takes a critical look at popular tools like Mem0 and Zep, and the findings are quite a 'WTF' moment, as the author puts it!

The core of the issue, according to the author, is that current LLM memory systems like Mem0 and Zep are surprisingly inefficient. Through a benchmark called MemBench, they discovered that these systems are not only 14 to 77 times more expensive but also 31 to 33 percent less accurate than just using a naive long-context approach. Can you believe that? That's a huge disconnect from what we often hear.

The culprit? An architecture dubbed 'LLM-on-Write.' Imagine this: every time your agent sends a message, these memory systems aren't just saving data. Oh no, they're spinning up multiple background LLM processes – sometimes three in parallel for Mem0 – just to extract meaning, summarize conversations, and check for contradictions. Zep's Graphiti system takes this even further, creating a recursive explosion of LLM calls, leading to an astonishing 1.17 million tokens burned per test case in their experiment. This 'N+1 latency and cost tax' means you're layering LLMs on top of LLMs, introducing noise and latency at every step.

A fatal flaw, they argue, is this reliance on LLMs for 'fact extraction.' Since these extractor LLMs are non-deterministic, they can introduce hallucinations right at the write time, corrupting your data before it even hits the database. No amount of retrieval optimization can fix that!

The article also points out a major issue with marketing, which tends to focus on 'cost per retrieval' while engineers and founders are actually paying for 'cost per conversation,' encompassing all these hidden LLM-on-Write expenses.

The big takeaway, and this is crucial for anyone building agents, is that 'Universal Memory' doesn't exist. The author makes a clear distinction between two types of memory: 'Semantic Memory' for the user – thinking about preferences, long-term history, and rapport, which can be fuzzy and graph-based; and 'Working Memory' for the agent – dealing with precise things like file paths, variable names, and immediate error logs, which must be lossless and exact. Trying to solve both with one tool, especially a semantic memory tool for working memory tasks, is an architectural mismatch. It's like trying to run a database on a lossy compression algorithm! So, if you're working with LLMs, remember to treat these memory types as separate systems with different requirements. It's a game-changer!

