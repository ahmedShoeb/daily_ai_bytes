---
layout: post 
title: "How We Made SWE-Bench 50x Smaller"
blog_url: https://logicstar.ai/blog/how-we-made-swe-bench-50x-smaller?utm_source=tldrai 
---



## Key Points

SWE-Bench Verified initially required 240 GiB of storage and took up to 30 hours to set up due to large Docker images and rate limits.
LogicStar AI restructured layers, trimmed unnecessary files, and compressed the results, shrinking SWE-Bench Verified from 240 GiB to 5 GiB.
This optimization reduces download and setup time to under a minute, facilitating large-scale evaluation and trace generation.
The 'layering problem' in Docker images, where near-duplicate layers added up, was a key issue.
The solution involved 'delta layering,' where each instance layer only adds the difference (delta) to the previous commit, eliminating duplication.
Git history and packfiles posed a challenge as new packfiles were treated as entirely new large files by Docker; this was addressed by creating one packfile per instance for additional git objects.
Unnecessary build artifacts like installers and caches were removed, further reducing image size.
Cross-layer compression using zstd was applied to the sorted layers, achieving a final archive size of 5 GiB.
The optimizations reduced SWE-Bench Verified to 31 GiB uncompressed and 5 GiB compressed, downloadable and unpackable in about five minutes.
The delta layering technique is generalizable and can be applied to other series of execution environments.

## Key Topics Discussed

The article details how LogicStar AI significantly reduced the size and setup time of SWE-Bench Verified, a benchmark for evaluating coding agents. Originally, SWE-Bench Verified was a cumbersome system, requiring 240 GiB of storage across hundreds of Docker images and taking up to 30 hours for initial setup due to large image sizes and Docker Hub rate limits. This made large-scale evaluation on cloud machines particularly inefficient.
LogicStar AI addressed this challenge through several key optimizations. The primary issue identified was the "layering problem" in Docker, where even minor changes between repository commits resulted in full copies of layers, leading to hundreds of near-duplicate layers and the massive 240 GiB footprint. To counteract this, they introduced "delta layering," a technique where each instance layer only stores the incremental differences from the preceding commit in a chronological chain. This dramatically reduced the duplication of repository snapshots.
Another complexity arose from Git history and packfiles. While Git efficiently stores changes, its use of packfiles meant that even small updates could trigger the creation of entirely new, multi-hundred-megabyte packfiles from Docker's perspective, negating the benefits of delta layering. To solve this, LogicStar AI restructured packfiles, generating one per instance for only the additional Git objects, accepting a slight loss in Git's internal compression for much smaller, incremental Docker layers.
Further size reductions were achieved by meticulously removing unnecessary build artifacts, such as Miniconda installers and Pip/Conda caches, which contributed gigabytes to each image but were not required at runtime. Finally, cross-layer compression using zstd was applied. By sorting layers chronologically, the compressor was highly effective at identifying and compressing repeated data across nearly identical layers, ultimately shrinking the entire benchmark from 240 GiB of raw images to a single 5 GiB archive.
These optimizations have transformed SWE-Bench Verified, making it downloadable and unpackable in approximately five minutes on modern machines. The core innovation of delta layering is also generalizable, allowing it to be applied to other series of execution environments beyond SWE-Bench. LogicStar AI has provided helper scripts on GitHub and hosts the compressed archive on Hugging Face for efficient distribution. The improvements are crucial not only for evaluation but also for generating high-quality training data for code models, enabling efficient trace generation on numerous ephemeral machines.

