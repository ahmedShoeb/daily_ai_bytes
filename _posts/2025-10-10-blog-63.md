---
layout: post 
title: "Gemini Robotics 1.5 brings AI agents into the physical world"
blog_url: https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/?utm_source=tldrai 
---



## Key Points

- Google DeepMind introduces Gemini Robotics 1.5 and Gemini Robotics-ER 1.5 to advance intelligent, general-purpose robots.
- Gemini Robotics 1.5 is a vision-language-action (VLA) model enabling robots to perceive, plan, think, use tools, and act for complex, multi-step tasks, and learns across different robot embodiments.
- Gemini Robotics-ER 1.5 is a vision-language model (VLM) specializing in embodied reasoning, tool-calling, and creating multi-step plans, achieving state-of-the-art spatial understanding.
- These models work together in an agentic framework, allowing robots to understand their environment and think before acting.
- Gemini Robotics-ER 1.5 is now available to developers via the Gemini API in Google AI Studio, while Gemini Robotics 1.5 is available to select partners.
- Google is committed to responsible AI development, implementing safety and alignment approaches, and upgrading the ASIMOV benchmark for semantic safety.

## Key Topics Discussed

Google DeepMind has unveiled Gemini Robotics 1.5 and Gemini Robotics-ER 1.5, marking a significant step towards bringing AI agents into the physical world. These models are designed to enable robots to perceive, plan, think, use tools, and act to solve complex, multi-step tasks, moving towards truly general-purpose intelligent robots. Gemini Robotics 1.5 functions as a vision-language-action (VLA) model, translating visual information and instructions into motor commands for robots. It allows robots to think transparently before acting and facilitates skill learning across various robot embodiments. Complementing this, Gemini Robotics-ER 1.5 is a vision-language model (VLM) focused on embodied reasoning, empowering robots to understand their physical environment, natively call digital tools like Google Search, and formulate detailed, multi-step plans to complete missions. This model has demonstrated state-of-the-art performance in spatial understanding benchmarks. Together, these models form an agentic framework, where Gemini Robotics-ER 1.5 orchestrates the robot's activities like a high-level brain, providing natural language instructions, which Gemini Robotics 1.5 then executes directly through its vision and language understanding. This innovative approach allows robots to generate internal reasoning sequences, generalize to longer tasks, and adapt to diverse environments more effectively. A key breakthrough is Gemini Robotics 1.5's ability to learn and transfer motions across different robot embodiments without needing specialized training for each new form factor. Google DeepMind emphasizes its commitment to responsible AI and Robotics development, proactively integrating novel safety and alignment approaches. This includes a holistic safety framework for Gemini Robotics 1.5, encompassing semantic reasoning, respectful human dialogue, and low-level safety sub-systems for collision avoidance. To further guide safe development, an upgraded ASIMOV benchmark for evaluating and improving semantic safety is also being released. These advancements are considered a foundational milestone towards solving Artificial General Intelligence (AGI) in the physical world, creating robots that can reason, plan, actively use tools, and generalize, ultimately becoming more helpful and integrated into human lives. Developers can access Gemini Robotics-ER 1.5 through the Gemini API in Google AI Studio, while Gemini Robotics 1.5 is available to select partners.

