---
layout: post 
title: "LoRA Without Regret"
blog_url: https://thinkingmachines.ai/blog/lora/?utm_source=tldrai 
---



## Key Points

LoRA (Low-Rank Adaptation) is a leading parameter-efficient fine-tuning (PEFT) method for large language models.
LoRA offers advantages in cost, speed, multi-tenant serving, training layout size, and ease of loading/transfer compared to full fine-tuning (FullFT).
The research demonstrates that LoRA can achieve the same ultimate performance and sample efficiency as FullFT under specific conditions.
For small-to-medium supervised fine-tuning datasets, LoRA performs similarly to FullFT.
LoRA underperforms when datasets exceed its capacity, leading to reduced training efficiency.
LoRA is less tolerant of large batch sizes than FullFT in some scenarios.
Optimal performance is achieved when LoRA is applied to all weight matrices, especially MLP and MoE layers, rather than just attention layers.
LoRA fully matches FullFT performance for reinforcement learning, even with very low ranks (e.g., rank 1).
The optimal learning rate for LoRA is consistently found to be approximately 10 times higher than that for FullFT.
LoRA offers a compute efficiency advantage, using slightly more than â…” of the FLOPs that full fine-tuning does per pass.

## Key Topics Discussed

The article "LoRA Without Regret" from Thinking Machines Lab explores Low-Rank Adaptation (LoRA), a prominent parameter-efficient fine-tuning (PEFT) method designed for large language models. It addresses the inefficiencies of full fine-tuning by proposing that post-training on smaller datasets should not require adjusting a model's entire trillion-parameter structure. LoRA achieves this by replacing original weight matrices with modified versions that use significantly fewer parameters, thus providing a low-dimensional representation of fine-tuning updates.

The authors detail several operational benefits of LoRA over traditional full fine-tuning (FullFT), including enhanced multi-tenant serving capabilities, reduced memory footprints during training, and simplified loading and transferring of model adapters. While LoRA's popularity has surged, its performance relative to FullFT has been a subject of ongoing discussion in the research community.

Through extensive supervised fine-tuning and reinforcement learning experiments conducted with Llama 3 and Qwen3 models on datasets such as Tulu3 and OpenThoughts3, the researchers pinpoint the critical conditions under which LoRA can match the efficiency and overall performance of FullFT. A key finding indicates that for small-to-medium-sized datasets used in instruction-tuning and reasoning, LoRA delivers performance comparable to FullFT. Conversely, LoRA's performance diminishes when the dataset size surpasses its parameter capacity, resulting in less efficient training.

Crucially, the study highlights that LoRA performs optimally when integrated across all weight matrices, particularly within the MLP and MoE layers, as opposed to being restricted to only attention layers, which showed a noticeable drop in performance. The research also uncovered that LoRA exhibits lower tolerance for large batch sizes compared to FullFT, a characteristic attributed to the distinct optimization dynamics of its product-of-matrices parametrization.

A significant revelation from the experiments is LoRA's exceptional efficacy in reinforcement learning (RL). Here, it consistently matches FullFT's learning performance, even when employing very low ranks. This outcome is explained by the inherently lower informational demands per episode in policy gradient RL methods.

The article further provides practical insights into LoRA's hyperparameters, noting an empirical observation that the optimal learning rate for LoRA is consistently around 10 times higher than that for FullFT. This finding offers a valuable guideline for practitioners. Furthermore, the authors demonstrate LoRA's compute efficiency advantage, estimating it utilizes approximately two-thirds fewer FLOPs than FullFT per pass, solidifying its position as a more efficient fine-tuning approach overall. The research aims to enhance the accessibility and customizability of fine-tuning, thereby contributing to a deeper theoretical understanding of model capacity and learning efficiency.

