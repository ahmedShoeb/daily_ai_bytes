---
layout: post 
title: "SOTA OCR on-device with Core ML and dots.ocr"
blog_url: https://huggingface.co/blog/dots-ocr-ne?utm_source=tldrai 
---



## Key Points

- dots.ocr is a 3B parameter OCR model from RedNote that achieves state-of-the-art on-device performance, surpassing Gemini 2.5 Pro in OmniDocBench.
- Running AI models on-device offers benefits like zero cost, no API keys, and offline capability, but requires careful consideration of compute and power limitations.
- Apple's Neural Engine, available in devices since 2017, is a highly power-efficient AI accelerator (12x more efficient than CPU, 4x more than GPU).
- Accessing the Neural Engine is primarily through Core ML, Apple's closed-source ML framework, which can make PyTorch model conversion challenging.
- This article is the first in a three-part series detailing the conversion of dots.ocr to run on-device using CoreML and MLX.
- The conversion process from PyTorch to CoreML involves capturing the PyTorch execution graph and compiling it into an `.mlpackage`.
- Model simplification, such as focusing on single-image processing and specific attention implementations, is crucial for successful conversion.
- Common conversion issues include data type mismatches (e.g., `torch.arange` outputting `int32`), `repeat_interleave` errors, and dynamic index problems related to masking logic.
- After initial conversion, the model may be excessively large (over 5GB) and slow (over a second per forward pass), necessitating further optimization like quantization and dynamic shapes for practical on-device deployment.

## Key Topics Discussed

The article introduces dots.ocr, a 3B parameter OCR model from RedNote, which achieves state-of-the-art performance on-device, outperforming Gemini 2.5 Pro in OmniDocBench. It highlights the growing feasibility and advantages of running competitive AI models directly on devices, emphasizing benefits such as cost reduction, elimination of API key management, and offline functionality. However, it also points out the necessity of considering limited compute and power resources for on-device deployment.
A significant focus of the article is Apple's Neural Engine, a specialized AI accelerator present in Apple devices since 2017, recognized for its high performance and superior power efficiency compared to CPUs and GPUs. The article explains that accessing the Neural Engine requires using Core ML, Apple's proprietary ML framework, which can pose challenges for developers attempting to convert models from other frameworks like PyTorch. To address this, the article outlines a three-part series that will detail the process of converting the dots.ocr model for on-device execution using a combination of Core ML and MLX, Apple's more flexible ML framework.
The article details the initial conversion steps from PyTorch to Core ML, involving the capture of the PyTorch execution graph and its compilation into an `.mlpackage`. It stresses the importance of simplifying the model before conversion, such as optimizing for single-image processing and standardizing attention implementations. The authors share their experience with common conversion hurdles, including data type inconsistencies, issues with `repeat_interleave` operations, and complexities arising from dynamic indexing in masking logic, offering insights into how these were resolved.
Despite a successful conversion, the article notes that the initial model might be impractically large (over 5GB) and computationally intensive, indicating the need for further optimization. The series will continue to explore the integration of Core ML and MLX to run the complete model on-device, followed by in-depth discussions on optimization techniques like quantization and dynamic shapes to enhance efficiency on the Neural Engine.

