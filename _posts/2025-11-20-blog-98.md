---
layout: post 
title: "Artificial Analysis Benchmarks: AA-Omniscience, Gemini 3 Pro, Inworld TTS 1 Max, IBM Granite 4.0, Reve V1, and GPT-5"
blog_url: https://threadreaderapp.com/thread/1990455484844003821.html?utm_source=tldrai 
---



## Key Points

Artificial Analysis introduced AA-Omniscience, a new benchmark for knowledge and hallucination across over 40 topics, revealing that most models are more likely to hallucinate than provide correct answers.
Claude 4.1 Opus leads in the Omniscience Index due to its low hallucination rate, while xAI's Grok 4 excels in Omniscience Accuracy.
Gemini 3 Pro Preview is announced as the new leader in the Artificial Analysis Intelligence Index, outperforming other models in several evaluations and demonstrating strong multimodal, coding, and agentic capabilities.
Gemini 3 Pro Preview has premium pricing ($2/$12 per million input/output tokens for <200K context) despite improved token efficiency, making it among the most expensive.
Inworld TTS 1 Max leads the Artificial Analysis Speech Arena Leaderboard, supporting 12 languages and voice cloning, with fast processing speeds.
IBM launched Granite 4.0, a new family of open-weights language models (3B to 32B), with Granite 4.0 H Small showing strong intelligence and token efficiency.
Reve V1 debuts at #3 in the Artificial Analysis Image Editing Leaderboard, offering single and multi-image editing, with free and Pro plan access.
OpenAI released GPT-5 with four reasoning effort configurations (High, Medium, Low, Minimal), showing significant differences in intelligence, token usage, speed, and cost, with 'High' setting a new intelligence frontier.
OpenAI also released gpt-oss-120b and gpt-oss-20b, open-weights models that are highly intelligent for their size and offer cost-efficient alternatives to proprietary APIs, with an Apache 2.0 license.

## Key Topics Discussed

This article from Artificial Analysis provides a comprehensive overview of several new AI model benchmarks and releases. A significant highlight is the introduction of AA-Omniscience, a new benchmark designed to evaluate knowledge and hallucination across a wide range of topics. This benchmark reveals that a majority of current models tend to hallucinate more often than they provide accurate answers, emphasizing the critical need for improved factual reliability in AI. Claude 4.1 Opus stands out in the Omniscience Index due to its superior performance in minimizing hallucinations, while xAI's Grok 4 leads in terms of overall accuracy.

The article also announces Gemini 3 Pro Preview as the new frontrunner in the Artificial Analysis Intelligence Index, surpassing previous leaders like OpenAI's GPT-5.1. Gemini 3 Pro demonstrates exceptional performance across various intelligence evaluations, including GPQA Diamond, MMLU-Pro, HLE, LiveCodeBench, and SciCode. It also showcases advanced coding and agentic capabilities, leading in two out of three coding evaluations and showing strength in agentic contexts. Furthermore, Gemini 3 Pro is a multimodal model, excelling in reasoning with image inputs as demonstrated by its top position in the MMMU-Pro benchmark. However, its premium pricing, at $2/$12 per million input/output tokens, positions it as one of the more expensive models to operate, despite improvements in token efficiency.

In the realm of Text-to-Speech, Inworld TTS 1 Max has taken the lead in the Artificial Analysis Speech Arena Leaderboard. This model supports 12 languages, offers voice cloning, and boasts efficient character processing speeds.

IBM has also entered the scene with the launch of Granite 4.0, a new family of open-weights language models ranging from 3B to 32B parameters. The Granite 4.0 H Small model, in particular, demonstrates a strong balance of intelligence and token efficiency, outperforming several other models in its category.

For image editing, Reve V1 by Reve AI has debuted at an impressive third place in the Artificial Analysis Image Editing Leaderboard. This model supports both single and multi-image edits and is accessible through a web app with free and pro plans, as well as an API Beta.

Finally, the article details OpenAI's release of GPT-5, which features four distinct reasoning effort configurations: High, Medium, Low, and Minimal. These configurations significantly impact the model's intelligence, token usage, speed, and cost. While the 'High' reasoning effort sets a new standard for AI intelligence, the 'Minimal' effort offers comparable intelligence to GPT-4.1 with much greater token efficiency. OpenAI also introduced gpt-oss-120b and gpt-oss-20b, open-weights models under an Apache 2.0 license. These models are lauded for their high intelligence relative to their size and sparsity, offering a cost-effective alternative to OpenAI's proprietary APIs. They are also designed for efficient deployment, with the 120B model runnable on a single NVIDIA H100 GPU.

