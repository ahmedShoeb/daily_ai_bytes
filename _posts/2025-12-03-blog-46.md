---
layout: post 
title: "Are we in a GPT-4-style leap that evals can't see?"
blog_url: https://martinalderson.com/posts/are-we-in-a-gpt4-style-leap-that-evals-cant-see/?utm_source=tldrai 
---



## Key Points

The author believes there's been another subtle 'GPT-4 moment' in AI, unnoticed by current evaluations.
Traditional chat-based evaluations are becoming obsolete; speed and specific capabilities are now more important.
Gemini 3 Pro Preview is highlighted for its exceptional design capabilities, enabling rapid and branded prototyping.
Anthropic's Opus 4.5 is praised for significantly improved software engineering prowess, requiring less oversight.
These advancements, particularly in design and robust engineering, suggest LLMs can now handle much more of the product development lifecycle.
Current LLM benchmarks are criticized for failing to assess qualitative aspects like 'design taste' or iterative workflow efficiency.
The author calls for new, more qualitative benchmarks to truly capture the progress and economic impact of LLMs.

## Key Topics Discussed

Hey everyone! Martin Alderson here shares a super interesting take on the current state of AI. He argues that we might be experiencing another significant leap, similar to the GPT-4 moment, but it's flying under the radar because our usual evaluation methods aren't catching it. Martin makes a strong case that those ad-hoc chat evaluations we've all gotten used to are pretty much maxed out. For daily use, speed is becoming a huge factor, and judging models purely on chat quality isn't giving us the full picture anymore.

He's particularly excited about Gemini 3 Pro Preview, which he says is absolutely incredible for design. Imagine being able to whip up prototypes that look genuinely impressive and perfectly match your brand â€“ it's like having a good designer right there with you, churning out iterations in minutes! This really changes the game for prototyping, letting you go from a concept to a 'go to market' idea much faster.

Then there's Anthropic's Opus 4.5. While not for design, Martin found its software engineering capabilities to be miles ahead. He can work with it for an hour or more without constant interruptions, making minor adjustments instead of fixing major blunders. It's like having a much less annoying software engineer on your team. He highlights that the combination of Gemini 3 Pro's design prowess and Opus 4.5's robust engineering means LLMs can now handle an order of magnitude more of the product development lifecycle.

But here's the kicker: Martin believes our current benchmarks are all wrong. They're great for testing knowledge retrieval, math, or science, but they completely miss qualitative aspects like 'design taste' or the ability to work iteratively without constant 'babysitting.' He urges the industry to start adding more qualitative benchmarks that reflect real-world usage, comparing our current evaluations to university exams rather than how the world actually works. This subtle shift, he argues, has profound implications, potentially closing the gap between benchmark scores and the hypothetical GDP growth from LLMs that many have found puzzling.

