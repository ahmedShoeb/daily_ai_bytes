---
layout: post 
title: "AGI Not Imminent: Why Current AI Approaches Lack Fundamental Cognitive Primitives"
blog_url: https://dlants.me/agi-not-imminent.html?utm_source=tldrai 
---



## Key Points

- The article challenges claims from OpenAI and Anthropic CEOs that human-level AI is imminent, arguing these are marketing statements rather than technical arguments
- Human cognition is built on evolutionary cognitive primitives like number sense, object permanence, causality, spatial navigation, and animate/inanimate distinction that are hardwired into vertebrate brains
- Language evolved on top of these cognitive primitives and assumes them implicitly, making it extremely difficult for LLMs to reverse-engineer this foundation from text alone
- Current transformer-based LLMs lack these cognitive primitives, explaining limitations like poor multi-digit arithmetic and difficulty with logical inference
- Even video training doesn't solve the core problem - models might learn statistical patterns but not true object permanence or persistent entity tracking
- Developmental psychology shows infants represent objects as bounded, cohesive entities - a fundamental representation that emerges from evolution and embodied experience
- Research like Google's SIMA 2 shows embodied training and language capabilities coexist separately in models but don't meaningfully interact to improve reasoning
- Alternative architectures like neurosymbolic AI or networks with feedback connections might be needed, but break the computational efficiencies that make transformers scalable
- Theoretical research shows transformers with chain-of-thought can theoretically solve problems in P (polynomial time), but whether they can actually learn to use this capacity is unknown
- Recent AAAI survey found 76% of AI researchers believe scaling current approaches is unlikely to achieve AGI, citing fundamental limitations in reasoning and generalization
