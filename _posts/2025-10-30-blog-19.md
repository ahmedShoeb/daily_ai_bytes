---
layout: post 
title: "MiniMax M2: The Agent and Code Model That's Redefining Cost-Efficiency and Speed"
blog_url: https://www.minimax.io/news/minimax-m2?utm_source=tldrai 
---



## Key Points

MiniMax has officially launched and open-sourced MiniMax M2, a new AI model designed for Agents and code.
M2 is significantly more cost-effective (8% of Claude Sonnet's price) and faster (twice the speed) than competitors, offered for free for a limited time.
It features top-tier coding capabilities for end-to-end development workflows and powerful agentic performance for complex, long-chain tool-calling tasks.
The model aims to solve the 'impossible triangle' challenge of balancing performance, price, and inference speed for Agent models.
MiniMax M2 demonstrates strong abilities in programming, tool use, and deep search, performing competitively with top overseas models.
The API price is set at $0.30 per million input tokens and $1.20 per million output tokens, with high TPS.
The free trial period for M2 is extended to November 7th, and complete model weights are open-sourced on Hugging Face for local deployment.
MiniMax has also launched M2-powered Agent products in 'Lightning Mode' and 'Pro Mode,' both currently available for free.

## Key Topics Discussed

Hey everyone, we've got some exciting news from MiniMax! They've officially launched and open-sourced their brand-new AI model, MiniMax M2, which is specifically engineered for Agents and code development. This release is a big step towards their vision of 'Intelligence with Everyone,' and it tackles a pretty common headache in the AI world: finding a model that truly balances performance, affordability, and lightning-fast inference speed. MiniMax noticed that current models either break the bank and are slow, or they just don't perform up to snuff, creating this 'impossible triangle' that M2 aims to solve. What's super impressive is M2's cost-effectiveness. It's priced at just 8% of Claude Sonnet's cost and runs nearly twice as fast, and get this, it's available for free for a limited time! This model isn't just about saving money; it's a powerhouse for developers. It boasts top-tier coding capabilities, perfect for those end-to-end development workflows, and incredibly powerful agentic performance. It can handle complex, multi-step tool-calling tasks with stable execution, integrating smoothly with tools like Shell, Browser, and Python interpreters. MiniMax has achieved this optimal balance of intelligence, speed, and cost through some seriously efficient design. When it comes to performance, M2's abilities in tool use and deep search are right up there with leading international models, and its programming skills are among the best you'll find domestically. The team at MiniMax has been putting M2 through its paces internally, using Agents to tackle everything from data analysis to HR processes, showing just how integrated and effective this model is. For those looking to integrate M2, the API is competitively priced, and they're offering an online inference service with high tokens per second. To sweeten the deal even further, they've extended the free trial until November 7th and have open-sourced the complete model weights on Hugging Face. This means you can deploy it locally, with support for vLLM and SGLang. But wait, there's more! MiniMax has also rolled out M2-powered Agent products. You've got 'Lightning Mode' for those quick, efficient outputs in conversational Q&A and simple coding tasks, and 'Pro Mode' for tackling more complex, in-depth research and full-stack development projects. Both of these MiniMax Agent products are also currently available for free! This whole launch really underscores MiniMax's belief that AGI is a powerful force for production, and Agents are the perfect vehicle to drive its advancement. So, if you're into AI and development, M2 is definitely something you'll want to check out!

