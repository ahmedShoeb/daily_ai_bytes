---
layout: post 
title: "LLM Compression: Co-training a Summarizer and Generator for Efficient Context Compaction"
blog_url: https://www.rajan.sh/llm-compression?utm_source=tldrai 
---



## Key Points

- An experiment co-trained a summarizer and a generator to learn an LLM compression scheme within the same token space as the base model.
- The goal is to predict next tokens efficiently with an order of magnitude fewer context tokens while maintaining full-context behavior.
- The model discovered unique compression tricks including aggressive pruning, dense punctuation (semicolons, em-dashes), and switching to Mandarin for higher information density.
- The setup involved a teacher model (full context), a generator (gist + local context), and a compressor (discrete code producer).
- A reward function balanced improved next-token prediction with a length penalty for the gist.
- Results showed summaries roughly 9.5% the size of the original corpus, with an order of magnitude reduction in KL divergence, indicating effective information preservation.
- The model successfully preserved numbers and proper nouns, a common challenge for other compression methods.
- The code for this experiment is available on GitHub.

## Key Topics Discussed

Welcome, podcast listeners! Today, we're diving into a fascinating experiment in Large Language Model, or LLM, context compression. This innovative research involved co-training a summarizer and a generator to develop a compression scheme that operates within the very same token space as the base model. The main goal? To enable LLMs to maintain their prediction capabilities with significantly fewer context tokens—we're talking an order of magnitude less—while staying remarkably close to their full-context performance.

What's truly exciting is that the model actually discovered its own unique compression strategies! These included aggressively pruning common words, using dense punctuation like semicolons and em-dashes, and even, surprisingly, switching to Mandarin tokens to pack more information into each token. This really highlights the model's learned efficiency in leveraging various linguistic features for optimal compression.

The experimental setup was quite clever, involving three key components: a teacher model that had access to the full context, a generator that predicted future tokens based on a compressed 'gist' and the last few tokens of the context, and a compressor that generated the discrete code for this gist. A sophisticated reward function was put in place to optimize this entire process, carefully balancing improvements in next-token prediction accuracy with a penalty for the length of the gist. This effectively created a practical trade-off between compression rate and distortion.

The results were impressive! The summarizer learned to create summaries that were approximately 9.5% the size of the original document. Crucially, the KL divergence between the decoder with the summarizer and the ground truth was reduced by an order of magnitude, meaning the core information was effectively preserved. A significant finding was the model's excellent preservation of numbers and proper nouns, which often pose challenges for other 'soft token' compression methods. The author also noted that the model's use of Mandarin tokens for compression was an unexpected and novel result, suggesting a deeper understanding of information density across languages. And for those of you who are technically inclined, the code for this groundbreaking experiment is openly available on GitHub!

