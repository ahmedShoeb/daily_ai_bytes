---
layout: post 
title: "Why are embeddings so cheap?"
blog_url: https://www.tensoreconomics.com/p/why-are-embeddings-so-cheap?utm_source=tldrai 
---



## Key Points

Embeddings are significantly cheaper than generative AI models like GPT or Gemini due to underlying low computational costs.
Modern embedding models are primarily compute-bound, meaning they don't benefit much from batching, unlike memory-bound autoregressive models.
The key hardware parameter for optimizing embedding API cost structure is FLOPS/dollar.
Processing a million input tokens for state-of-the-art embedding models can cost less than 1Â¢ with sufficient demand and scale.
Real-world benchmarks show limited throughput improvement with increased batch size for embeddings, leading to higher latency for individual users.
Analysis reveals that most of the execution time for embedding generation is spent in large-scale matrix multiplication kernels.
GPUs like the RTX 4090 offer a better FLOPS/dollar ratio than H100s for embedding workloads, leading to even cheaper token processing costs.
This cost efficiency and convergence of models to similar semantic representations make serving raw embeddings a challenging business.

## Key Topics Discussed

Hey everyone, and welcome back to the podcast! Today, we're diving into a fascinating topic from Tensor Economics: 'Why are embeddings so cheap?' It's a question many of us have pondered, especially when comparing their costs to the hefty price tags of generative models like GPT or Gemini. The article breaks down that embeddings are incredibly cost-efficient because the underlying computations are just remarkably cheap. Unlike generative models, which are often memory-bound, modern embedding models are primarily compute-bound. This is a crucial distinction because it means that while you might expect big gains from batching requests, for embeddings, those benefits are pretty limited, and often come with a trade-off in increased latency for individual users. The core insight here is that optimizing the cost structure for an embedding API boils down to maximizing FLOPS per dollar. The article details how the majority of the time spent generating an embedding is in large-scale matrix multiplications. They estimate that with enough demand and scale, the cost to process a million input tokens can even drop below one cent for leading embedding models! They also perform a deep dive into different GPU architectures, comparing powerhouses like the H100 with more consumer-grade options like the RTX 4090. Surprisingly, for embedding tasks, the RTX 4090 actually delivers a better FLOPS per dollar ratio. This means you could potentially process tokens even more cheaply on a 4090 than an H100, which is pretty wild! Ultimately, the article concludes that the low cost of embeddings, combined with the fact that many leading models are converging on similar semantic representations, creates a fiercely competitive market. This 'intelligence involution,' as they call it, foreshadows a future where even generative model costs might follow a similar downward trend.

