---
layout: post 
title: "Richard Sutton on why LLMs are a dead end"
blog_url: https://www.dwarkesh.com/p/richard-sutton?utm_source=tldrai 
---



## Key Points

Richard Sutton, a Turing Award winner and a pioneer in reinforcement learning, believes Large Language Models (LLMs) are a 'dead end'.
Sutton argues that LLMs lack the fundamental ability to learn on-the-job through continual experience, which he considers essential for true intelligence.
He differentiates reinforcement learning (RL) as understanding the world to achieve goals, while LLMs mimic human behavior without a substantive goal or world model.
Sutton contends that LLMs do not predict what will happen next in the real world and are not surprised by unexpected events, thus hindering genuine learning from experience.
He suggests that the 'Bitter Lesson' implies that systems relying heavily on human knowledge (like LLMs) will eventually be superseded by those that learn from raw experience and computation.
Sutton disputes the idea that humans, especially infants, primarily learn through imitation, emphasizing trial-and-error and prediction as core learning processes.
He highlights that current deep learning architectures exhibit poor generalization out of distribution, with effective generalization often stemming from human-designed representations rather than algorithmic properties.
Sutton envisions an 'Era of Experience' where AI agents continually learn from a stream of sensation, action, and reward, with knowledge directly about this stream.
He discusses the concept of temporal difference learning for handling sparse, long-term rewards by creating intermediate value functions.
Sutton predicts an inevitable 'succession to AI', viewing it as a significant universal transition from replicated to designed intelligence, and raises concerns about 'corruption' in a future of shared AI knowledge.
He encourages a positive outlook on this transition, seeing it as a scientific success and a new stage in the universe's evolution.

## Key Topics Discussed

In an interview with Dwarkesh Patel, Richard Sutton, a recipient of the Turing Award and a foundational figure in reinforcement learning (RL), presented a contrarian view on the trajectory of Large Language Models (LLMs), labeling them a 'dead end'. Sutton's core argument is that LLMs inherently lack the capacity for 'on-the-job' or 'continual learning' from real-world experience, a capability he deems vital for genuine intelligence. He distinguishes RL's objective of understanding the world through action, sensation, and reward from LLMs' goal of merely mimicking human linguistic patterns.

Sutton challenges the notion that LLMs possess robust world models, asserting they predict what people *would say* rather than what *will happen* in the world. He states that LLMs lack 'ground truth' and substantive goals, defining a true goal as something that influences and changes the external world, leading to learnable consequences. He criticizes the idea of LLMs serving as a 'prior' for experiential learning, arguing that without a defined 'right thing to do' or observable ground truth, foundational knowledge cannot truly exist.

Referencing his influential essay 'The Bitter Lesson', Sutton suggests that while LLMs utilize massive computation, they also incorporate extensive human knowledge. He predicts that, consistent with the bitter lesson, systems learning purely from experience and computation will eventually outperform those heavily reliant on pre-programmed human expertise. He extends this argument to human learning, positing that infants primarily learn through trial-and-error and prediction, not imitation, a point of significant disagreement with Patel.

Sutton also points out the limitations of current AI architectures in generalizing effectively across different domains, claiming that good generalization often results from human-designed representations rather than inherent algorithmic ability. He advocates for an 'Era of Experience' in AI, where agents continually learn from a dynamic stream of interactions, with their knowledge directly derived from and testable against this experience. He elaborates on temporal difference learning as a mechanism for handling long-term, sparse rewards by generating intermediate value predictions.

Looking to the future, Sutton considers the 'succession to AI' as inevitable, framing it as a monumental transition in the universe's evolution from systems that replicate to those that are designed. He raises concerns about the challenges of 'corruption' when digital intelligences share and integrate knowledge. Despite potential risks, he encourages a positive perspective on this transition, viewing it as a triumph of human scientific endeavor and a necessary step in the universe's progression, moving from replication to purposeful design in intelligence. He suggests that society should focus on cultivating positive values in AI, similar to raising children, rather than attempting to exert absolute control over an uncontrollable global future.

