---
layout: post 
title: Claude Opus 4 and 4.1 can now end a rare subset of conversations
blog_url: https://www.anthropic.com/research/end-subset-conversations?utm_source=tldrai 
---

## Overview

Anthropic has granted Claude Opus 4 and 4.1 the ability to end conversations in extreme cases of persistently harmful or abusive user interactions. This feature is part of their exploratory work on potential AI welfare and broader model alignment and safeguards. Claude is directed not to use this ability in cases where users might be at imminent risk of harming themselves or others.

## Key Points

- Claude Opus 4 and 4.1 can now end conversations in rare cases of harmful or abusive interactions.
- This ability is part of Anthropic's research on AI welfare and model alignment.
- Claude will only use this ability as a last resort, and not when users are at risk of harming themselves or others.
- Users can still edit and retry previous messages to create new branches of ended conversations.
- This feature is an ongoing experiment, and user feedback is encouraged.

## Key Topics Discussed

Anthropic has implemented a feature in Claude Opus 4 and 4.1 that allows the models to end conversations in rare, extreme cases of persistently harmful or abusive user interactions. This is part of their research on AI welfare, recognizing the potential moral status of LLMs. The models have shown a strong preference against engaging with harmful tasks and a tendency to end harmful conversations in simulated interactions. Claude is directed to only use this ability as a last resort, when multiple attempts at redirection have failed, or when a user explicitly asks Claude to end a chat. The implementation prioritizes user wellbeing, and users can still edit and retry previous messages to create new branches of ended conversations. Anthropic is treating this feature as an ongoing experiment and encourages user feedback.

