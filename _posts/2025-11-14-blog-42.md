---
layout: post 
title: "RL ENVs & Real-World Agents: How Close Are We?"
blog_url: https://surgehq.ai/blog/rl-envs-real-world?utm_source=tldrai 
---



## Key Points

2025 marks a shift towards AI agents performing multi-step real-world tasks within Reinforcement Learning (RL) environments.
Testing nine AI models in an RL environment revealed GPT-5 and Claude Sonnet 4.5 as top performers, but they still failed over 40% of tasks.
Effective RL environments are organically 
grown

 incorporating a world model, entities, and tools, grounded in real worker experience.
A 
Hierarchy of Agentic Capabilities

 is proposed: Basic Tool Use, Planning & Goal Formation, Adaptability, Groundedness, and Common Sense Reasoning.
Lower-tier models often struggle with fundamental tasks like correct tool usage and breaking down multi-step objectives.
Intermediate models demonstrate planning but lack adaptability when plans encounter unexpected real-world issues.
Advanced models still exhibit problems with 
groundedness

 occasionally losing context or hallucinating information.
Even top models like GPT-5 show limitations in common sense reasoning, misinterpreting tasks or failing to connect obvious dots.
Achieving proficiency in foundational agentic capabilities is a prerequisite for analyzing and developing true common sense reasoning in AI.
The current stage focuses on developing reliably coherent agents, with the next major challenge being to close the gap in common sense reasoning for human-level performance.

## Key Topics Discussed

Welcome to another episode of our tech podcast! Today, we're diving into a fascinating topic: how close are we to having AI agents that can truly operate in the real world? The year 2025 has seen a huge shift, with AI moving out of just being a chatbot and into becoming agents capable of performing complex, multi-step tasks in virtual environments. This means the focus is now squarely on Reinforcement Learning, or RL, environments for training and evaluating these agents.

Surge HQ conducted a revealing study where they 'hired' nine different AI models to tackle 150 tasks within one of their RL environments. The big takeaway? While models like GPT-5 and Claude Sonnet 4.5 are definitely leading the pack, even they still failed on over 40% of the tasks. This really highlights the challenges we're facing in getting these agents to human-level performance.

The article explains that for these RL environments to be truly effective, they can't just be abstract simulations. They need to be 'grown' organically, built with a coherent world model, diverse entities, and a robust tool system. Crucially, they should be grounded in real worker experiences, meaning the same kinds of people the agents are meant to work alongside are actually shaping these virtual worlds. For example, one environment, Corecraft, Inc., simulates an online PC parts retailer, where AI agents take on the role of customer support, handling everything from simple lookups to complex operational workflows.

From analyzing how these models performed, a clear 'Hierarchy of Agentic Capabilities' emerged. At the very foundation, we have basic tool use, goal formation, and planning. Lower-tier models, like GPT-4o, Mistral Medium, and Nova Pro, consistently stumbled here, making basic errors in using tools or failing to break down tasks into logical steps. They're more like chatbots with tools, rather than true agents.

Moving up, the next capability is 'Adaptability.' This is about an agent's ability to adjust its plan when reality doesn't quite cooperate. Models like Gemini 2.5 and Qwen3, while good at planning, often failed to adapt when a step in their plan went wrong. They’d take empty search results at face value instead of trying a different approach. Claude Sonnet 4.5, however, showed impressive adaptability, actively trying various search parameters when it hit a roadblock—just like a human would.

Then there's 'Groundedness,' which is all about staying tethered to the current context, avoiding hallucinations, or inventing facts. Kimi K2 Turbo, for instance, frequently got the year wrong in its tool calls, and even Claude Sonnet 4.5, despite its strengths, sometimes lost context, though it often recovered. These subtle grounding issues can be tricky because they can slip into the final answers unnoticed.

And finally, at the very top, is 'Common Sense Reasoning.' This is where even the best models, including GPT-5, currently fall short. It's that 'general' in 'general intelligence'—the ability to reason sensibly in unfamiliar situations, connect the dots, and understand nuanced interpretations. GPT-5 occasionally missed simple inferences or chose inefficient plans, like manually sifting through orders day by day instead of using a more sensible, overarching search strategy. It even misinterpreted an instruction to mean changing an account name rather than identifying the customer, despite clear contextual clues.

So, what does all this mean? Well, 2025 isn't the year we achieved human-level general-purpose agents. Instead, it's the year we developed agents coherent enough that we can really start to analyze and discuss their common sense reasoning. The article concludes that achieving high proficiency in all these foundational agentic capabilities is essential. But the biggest challenge ahead, the one that will truly shape the next stage of AI development, is figuring out how to close that gap in common sense reasoning. It's an open question how long that will take, but it's an incredibly exciting frontier for AI.

